{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired/based on https://github.com/rayidghani/magicloops and https://github.com/dssg/MLforPublicPolicy/blob/master/labs/2019/lab6_feature_generation_sol.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_v2 as pipeline\n",
    "import importlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "datafile = \"projects_2012_2013.csv\"\n",
    "data = pd.read_csv(datafile, parse_dates=['date_posted', 'datefullyfunded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "stats_for_variables = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]\n",
    "\n",
    "variables_for_correlation = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]\n",
    "\n",
    "pipeline.explore_data(data, stats_for_variables, variables_for_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see in the histograms that total_price_including_optional_support and students_reached have very big outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create outcome label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "# The task is to predict if a project will not get fully funded within 60 days of posting.\n",
    "data['duration_of_funding'] = data.datefullyfunded - data.date_posted\n",
    "data['not_funded_in_60'] =  np.where(data['duration_of_funding']<=pd.Timedelta('60 days'), 1, 0)\n",
    "output_label ='not_funded_in_60'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create binary features for categorical data, discretize features for contiuous data, and an aggregation feature\n",
    "\n",
    "#Select columns from which we will create binary features. We use string string columns who have less than 50 different values (we dont want to generate too many binary values)\n",
    "str_columns = [column for column in data.columns if data[column].dtype=='object' and len(data[column].unique())<51]\n",
    "\n",
    "#Columns with float values to generate discrete features\n",
    "float_columns = ['total_price_including_optional_support', 'students_reached']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#Generate the binary features\n",
    "features = pipeline.create_dummies(data[str_columns], str_columns)\n",
    "\n",
    "#Genereate discretized features. Using qcut due to outliers (if not, almost all datapoints end up in 'low')\n",
    "for float_column in float_columns:\n",
    "  features[float_column] = pd.qcut(data[float_column], 5, labels=['low', 'medium low', 'medium', 'medium high', 'high'])\n",
    "  \n",
    "# Generate binary features for the new discretized columns\n",
    "features = pipeline.create_dummies(features, float_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate feature: number of projects that got funded in the last 10 days. Idea: if many projects have been funded lately, this could be good info to predictor if a project will be funded soon\n",
    "\n",
    "#List of all dates where projects have been posted\n",
    "date_posted_list = pd.to_datetime(data['date_posted'].unique())\n",
    "\n",
    "#We use a dictionary to save the amount of projects that have been funded within the last 10 days in each specific day\n",
    "num_projects_funded_dict = {}\n",
    "\n",
    "#For every possible date_posted\n",
    "for date_posted in date_posted_list:\n",
    "  #For each project, we calculate the difference between the current observed date and the project funded date\n",
    "  #Lets remember that the difference between a value (date_posted) and a series (data['datefullyfunded']) is a series\n",
    "  diff_date_and_fully_funded = date_posted - data['datefullyfunded']\n",
    "  \n",
    "  #Count how many projects have a difference between fully funded date and current date bigger than 0 and smaller or equal than 10\n",
    "  amount_funded_in_last_10_days = np.sum((diff_date_and_fully_funded>pd.Timedelta('0 days')) & (diff_date_and_fully_funded<=pd.Timedelta('10 days')))\n",
    "  \n",
    "  #Save the amount in dictionary\n",
    "  num_projects_funded_dict[date_posted.strftime(\"%Y%m%d\")]= amount_funded_in_last_10_days\n",
    "\n",
    "#We create the column to be attached, initially full of zeros for each row in the dataframe\n",
    "num_of_projects_funded_10_days = np.zeros(len(data))\n",
    "for index, row in data.iterrows():\n",
    "  num_of_projects_funded_10_days[index] = num_projects_funded_dict[row['date_posted'].strftime(\"%Y%m%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach the created column to features\n",
    "features['num_of_projects_funded_10_days']=num_of_projects_funded_10_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create temporal train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#Create three sets of train and test data, based on threee different split thresholds\n",
    "split_thresholds = [pd.Timestamp(2013,7,1), pd.Timestamp(2013,1,1), pd.Timestamp(2012,7,1)]\n",
    "\n",
    "#Indicating which is the column to be used for splitting training and test daata\n",
    "date_column='date_posted'\n",
    "\n",
    "#Amount of data used for test set\n",
    "test_window = relativedelta(months=6)\n",
    "\n",
    "#Gap needed between training and test set. 60 days in this case\n",
    "gap_training_test = relativedelta(days=100)\n",
    "\n",
    "#Generate train and test sets\n",
    "train_test_sets= pipeline.create_temp_validation_train_and_testing_sets(\n",
    "  data,\n",
    "  features,\n",
    "  date_column,\n",
    "  output_label,\n",
    "  split_thresholds,\n",
    "  test_window,\n",
    "  gap_training_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "columns_to_process = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Clasifiers and parameters generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#We define the specific models we want to run\n",
    "models_to_run=['LR']#DT']#LR']#DT','LR']#,'AB']#,'RF','KNN']#,'BA','SVM']\n",
    "\n",
    "#Get all posible models and their different sets of parameters\n",
    "models, parameters_grid = pipeline.get_models_and_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over models and different training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "results = pipeline.iterate_over_models_and_training_test_sets(models_to_run, models, parameters_grid, train_test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
