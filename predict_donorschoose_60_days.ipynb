{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired/based on https://github.com/rayidghani/magicloops and https://github.com/dssg/MLforPublicPolicy/blob/master/labs/2019/lab6_feature_generation_sol.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_v2 as pipeline\n",
    "import importlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "datafile = \"projects_2012_2013.csv\"\n",
    "data = pd.read_csv(datafile, parse_dates=['date_posted', 'datefullyfunded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exploration...\n",
      "\n",
      "Number of rows: 124976\n",
      "\n",
      "Columns and types of data:\n",
      "projectid                                         object\n",
      "teacher_acctid                                    object\n",
      "schoolid                                          object\n",
      "school_ncesid                                    float64\n",
      "school_latitude                                  float64\n",
      "school_longitude                                 float64\n",
      "school_city                                       object\n",
      "school_state                                      object\n",
      "school_metro                                      object\n",
      "school_district                                   object\n",
      "school_county                                     object\n",
      "school_charter                                    object\n",
      "school_magnet                                     object\n",
      "teacher_prefix                                    object\n",
      "primary_focus_subject                             object\n",
      "primary_focus_area                                object\n",
      "secondary_focus_subject                           object\n",
      "secondary_focus_area                              object\n",
      "resource_type                                     object\n",
      "poverty_level                                     object\n",
      "grade_level                                       object\n",
      "total_price_including_optional_support           float64\n",
      "students_reached                                 float64\n",
      "eligible_double_your_impact_match                 object\n",
      "date_posted                               datetime64[ns]\n",
      "datefullyfunded                           datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Statistics for selected variables:\n",
      "count    124976.000000\n",
      "mean        654.011811\n",
      "std        1098.015854\n",
      "min          92.000000\n",
      "25%         345.810000\n",
      "50%         510.500000\n",
      "75%         752.960000\n",
      "max      164382.840000\n",
      "Name: total_price_including_optional_support, dtype: float64\n",
      "Number of outliers (>4 standard dev):521\n",
      "\n",
      "\n",
      "count    124917.000000\n",
      "mean         95.445760\n",
      "std         163.481912\n",
      "min           1.000000\n",
      "25%          23.000000\n",
      "50%          30.000000\n",
      "75%         100.000000\n",
      "max       12143.000000\n",
      "Name: students_reached, dtype: float64\n",
      "Number of outliers (>4 standard dev):2063\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Correlation between ['total_price_including_optional_support', 'students_reached']\n",
      "0.1766274350291622\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "stats_for_variables = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]\n",
    "\n",
    "variables_for_correlation = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]\n",
    "\n",
    "pipeline.explore_data(data, stats_for_variables, variables_for_correlation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see in the histograms that total_price_including_optional_support and students_reached have very big outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create outcome label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "# The task is to predict if a project onprint(sklearn.__version__) donorschoose will not get fully funded within 60 days of posting.\n",
    "data['duration_of_funding'] = data.datefullyfunded - data.date_posted\n",
    "data['not_funded_in_60'] =  np.where(data['duration_of_funding']>pd.Timedelta('60 days'), 1, 0)\n",
    "# label ='not_funded_in_60'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create binary features for categorical data, discretize features for contiuous data, and an aggregation feature\n",
    "\n",
    "#Select columns from which we will create binary features. We use string string columns who have less than 50 different values (we dont want to generate too many binary values)\n",
    "str_columns = [column for column in data.columns if data[column].dtype=='object' and len(data[column].unique())<51]\n",
    "\n",
    "#Columns with float values to generate discrete features\n",
    "float_columns = ['total_price_including_optional_support', 'students_reached']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_metro_rural</th>\n",
       "      <th>school_metro_suburban</th>\n",
       "      <th>school_metro_urban</th>\n",
       "      <th>school_metro_nan</th>\n",
       "      <th>school_charter_f</th>\n",
       "      <th>school_charter_t</th>\n",
       "      <th>school_charter_nan</th>\n",
       "      <th>school_magnet_f</th>\n",
       "      <th>school_magnet_t</th>\n",
       "      <th>school_magnet_nan</th>\n",
       "      <th>...</th>\n",
       "      <th>total_price_including_optional_support_medium</th>\n",
       "      <th>total_price_including_optional_support_medium high</th>\n",
       "      <th>total_price_including_optional_support_high</th>\n",
       "      <th>total_price_including_optional_support_nan</th>\n",
       "      <th>students_reached_low</th>\n",
       "      <th>students_reached_medium low</th>\n",
       "      <th>students_reached_medium</th>\n",
       "      <th>students_reached_medium high</th>\n",
       "      <th>students_reached_high</th>\n",
       "      <th>students_reached_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   school_metro_rural  school_metro_suburban  school_metro_urban  \\\n",
       "0                   0                      0                   1   \n",
       "1                   0                      0                   1   \n",
       "2                   0                      0                   1   \n",
       "3                   0                      0                   1   \n",
       "4                   0                      1                   0   \n",
       "\n",
       "   school_metro_nan  school_charter_f  school_charter_t  school_charter_nan  \\\n",
       "0                 0                 1                 0                   0   \n",
       "1                 0                 1                 0                   0   \n",
       "2                 0                 1                 0                   0   \n",
       "3                 0                 1                 0                   0   \n",
       "4                 0                 1                 0                   0   \n",
       "\n",
       "   school_magnet_f  school_magnet_t  school_magnet_nan          ...           \\\n",
       "0                1                0                  0          ...            \n",
       "1                1                0                  0          ...            \n",
       "2                1                0                  0          ...            \n",
       "3                0                1                  0          ...            \n",
       "4                1                0                  0          ...            \n",
       "\n",
       "   total_price_including_optional_support_medium  \\\n",
       "0                                              0   \n",
       "1                                              0   \n",
       "2                                              0   \n",
       "3                                              0   \n",
       "4                                              0   \n",
       "\n",
       "   total_price_including_optional_support_medium high  \\\n",
       "0                                                  0    \n",
       "1                                                  0    \n",
       "2                                                  0    \n",
       "3                                                  0    \n",
       "4                                                  0    \n",
       "\n",
       "   total_price_including_optional_support_high  \\\n",
       "0                                            1   \n",
       "1                                            0   \n",
       "2                                            1   \n",
       "3                                            0   \n",
       "4                                            1   \n",
       "\n",
       "   total_price_including_optional_support_nan  students_reached_low  \\\n",
       "0                                           0                     0   \n",
       "1                                           0                     0   \n",
       "2                                           0                     0   \n",
       "3                                           0                     0   \n",
       "4                                           0                     0   \n",
       "\n",
       "   students_reached_medium low  students_reached_medium  \\\n",
       "0                            0                        1   \n",
       "1                            0                        1   \n",
       "2                            0                        0   \n",
       "3                            1                        0   \n",
       "4                            0                        0   \n",
       "\n",
       "   students_reached_medium high  students_reached_high  students_reached_nan  \n",
       "0                             0                      0                     0  \n",
       "1                             0                      0                     0  \n",
       "2                             1                      0                     0  \n",
       "3                             0                      0                     0  \n",
       "4                             0                      1                     0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#Generate the binary features\n",
    "features = pipeline.create_dummies(data[str_columns], str_columns)\n",
    "\n",
    "#Genereate discretized features. Using qcut due to outliers (if not, almost all datapoints end up in 'low')\n",
    "for float_column in float_columns:\n",
    "  features[float_column] = pd.qcut(data[float_column], 5, labels=['low', 'medium low', 'medium', 'medium high', 'high'])\n",
    "  \n",
    "# Generate binary features for the new discretized columns\n",
    "features = pipeline.create_dummies(features, float_columns)\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate feature: number of projects that got funded in the last 10 days. Idea: if many projects have been funded lately, this could be good info to predictor if a project will be funded soon\n",
    "\n",
    "#List of all dates where projects have been posted\n",
    "date_posted_list = pd.to_datetime(data['date_posted'].unique())\n",
    "\n",
    "#We use a dictionary to save the amount of projects that have been funded within the last 10 days in each specific day\n",
    "num_projects_funded_dict = {}\n",
    "\n",
    "#For every possible date_posted\n",
    "for date_posted in date_posted_list:\n",
    "  #For each project, we calculate the difference between the current observed date and the project funded date\n",
    "  #Lets remember that the difference between a value (date_posted) and a series (data['datefullyfunded']) is a series\n",
    "  diff_date_and_fully_funded = date_posted - data['datefullyfunded']\n",
    "  \n",
    "  #Count how many projects have a difference between fully funded date and current date bigger than 0 and smaller or equal than 10\n",
    "  amount_funded_in_last_10_days = np.sum((diff_date_and_fully_funded>pd.Timedelta('0 days')) & (diff_date_and_fully_funded<=pd.Timedelta('10 days')))\n",
    "  \n",
    "  #Save the amount in dictionary\n",
    "  num_projects_funded_dict[date_posted.strftime(\"%Y%m%d\")]= amount_funded_in_last_10_days\n",
    "\n",
    "#We create the column to be attached, initially full of zeros for each row in the dataframe\n",
    "num_of_projects_funded_10_days = np.zeros(len(data))\n",
    "for index, row in data.iterrows():\n",
    "  num_of_projects_funded_10_days[index] = num_projects_funded_dict[row['date_posted'].strftime(\"%Y%m%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach the created column to features\n",
    "features['num_of_projects_funded_10_days']=num_of_projects_funded_10_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create temporal train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#Create three sets of train and test data, based on threee different split thresholds\n",
    "split_thresholds = [pd.Timestamp(2013,7,1),\n",
    "                    pd.Timestamp(2013,1,1),\n",
    "                    pd.Timestamp(2012,7,1)]\n",
    "\n",
    "#Amount of data used for test set\n",
    "test_window = relativedelta(months=6)\n",
    "\n",
    "#Generate train and test sets\n",
    "train_test_sets= pipeline.create_temp_validation_train_and_testing_sets(\n",
    "  data,\n",
    "  features,\n",
    "  'date_posted',\n",
    "  'not_funded_in_60',\n",
    "  split_thresholds,\n",
    "  test_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "columns_to_process = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Clasifiers and parameters generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#We define the specific models we want to run\n",
    "\n",
    "#Not running BA and KNN because taking too long\n",
    "\n",
    "models_to_run=['LR']#,'DT','LR','AB','RF','SVM']#,'BA','KNN']\n",
    "\n",
    "#Get all posible models and their different sets of parameters\n",
    "models, parameters_grid = pipeline.get_models_and_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over models and different training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With first train set (biggest one):\n",
      "Running LR...\n",
      "Running LR...\n",
      "Running LR...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>train_test_split_threshold</th>\n",
       "      <th>p_at_1</th>\n",
       "      <th>r_at_1</th>\n",
       "      <th>f1_at_1</th>\n",
       "      <th>p_at_2</th>\n",
       "      <th>r_at_2</th>\n",
       "      <th>f1_at_2</th>\n",
       "      <th>...</th>\n",
       "      <th>p_at_20</th>\n",
       "      <th>r_at_20</th>\n",
       "      <th>f1_at_20</th>\n",
       "      <th>p_at_30</th>\n",
       "      <th>r_at_30</th>\n",
       "      <th>f1_at_30</th>\n",
       "      <th>p_at_50</th>\n",
       "      <th>r_at_50</th>\n",
       "      <th>f1_at_50</th>\n",
       "      <th>auc-roc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=0.01, class_weight=None, ...</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.017261</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.484711</td>\n",
       "      <td>0.034044</td>\n",
       "      <td>0.063619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429978</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.354870</td>\n",
       "      <td>0.415774</td>\n",
       "      <td>0.438196</td>\n",
       "      <td>0.426690</td>\n",
       "      <td>0.387085</td>\n",
       "      <td>0.679924</td>\n",
       "      <td>0.493320</td>\n",
       "      <td>0.668514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=0.01, class_weight=None, ...</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0.623256</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.038215</td>\n",
       "      <td>0.598608</td>\n",
       "      <td>0.037952</td>\n",
       "      <td>0.071379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498494</td>\n",
       "      <td>0.316564</td>\n",
       "      <td>0.387224</td>\n",
       "      <td>0.478147</td>\n",
       "      <td>0.455428</td>\n",
       "      <td>0.466511</td>\n",
       "      <td>0.432635</td>\n",
       "      <td>0.686820</td>\n",
       "      <td>0.530870</td>\n",
       "      <td>0.683533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=0.01, class_weight=None, ...</td>\n",
       "      <td>{'C': 0.01, 'penalty': 'l1'}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.457317</td>\n",
       "      <td>0.017779</td>\n",
       "      <td>0.034227</td>\n",
       "      <td>0.448171</td>\n",
       "      <td>0.034847</td>\n",
       "      <td>0.064665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391655</td>\n",
       "      <td>0.304848</td>\n",
       "      <td>0.342842</td>\n",
       "      <td>0.373363</td>\n",
       "      <td>0.435937</td>\n",
       "      <td>0.402231</td>\n",
       "      <td>0.348499</td>\n",
       "      <td>0.678203</td>\n",
       "      <td>0.460412</td>\n",
       "      <td>0.662038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name                                              model  \\\n",
       "0         LR  LogisticRegression(C=0.01, class_weight=None, ...   \n",
       "1         LR  LogisticRegression(C=0.01, class_weight=None, ...   \n",
       "2         LR  LogisticRegression(C=0.01, class_weight=None, ...   \n",
       "\n",
       "                     parameters train_test_split_threshold    p_at_1  \\\n",
       "0  {'C': 0.01, 'penalty': 'l1'}                 2013-07-01  0.492063   \n",
       "1  {'C': 0.01, 'penalty': 'l1'}                 2013-01-01  0.623256   \n",
       "2  {'C': 0.01, 'penalty': 'l1'}                 2012-07-01  0.457317   \n",
       "\n",
       "     r_at_1   f1_at_1    p_at_2    r_at_2   f1_at_2    ...      p_at_20  \\\n",
       "0  0.017261  0.033351  0.484711  0.034044  0.063619    ...     0.429978   \n",
       "1  0.019712  0.038215  0.598608  0.037952  0.071379    ...     0.498494   \n",
       "2  0.017779  0.034227  0.448171  0.034847  0.064665    ...     0.391655   \n",
       "\n",
       "    r_at_20  f1_at_20   p_at_30   r_at_30  f1_at_30   p_at_50   r_at_50  \\\n",
       "0  0.302100  0.354870  0.415774  0.438196  0.426690  0.387085  0.679924   \n",
       "1  0.316564  0.387224  0.478147  0.455428  0.466511  0.432635  0.686820   \n",
       "2  0.304848  0.342842  0.373363  0.435937  0.402231  0.348499  0.678203   \n",
       "\n",
       "   f1_at_50   auc-roc  \n",
       "0  0.493320  0.668514  \n",
       "1  0.530870  0.683533  \n",
       "2  0.460412  0.662038  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "print(\"With first train set (biggest one):\")\n",
    "pipeline.iterate_over_models_and_training_test_sets(models_to_run, models, parameters_grid, train_test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With second train set:\")\n",
    "pipeline.iterate_over_models(models_to_run, models, parameters_grid, x_train_2, x_test_2, y_train_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With third train set (smalles one):\n",
      "Running LR...\n",
      "Running DT...\n",
      "Running LR...\n",
      "Running AB...\n",
      "Running RF...\n",
      "Running SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipe/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>p_at_1</th>\n",
       "      <th>r_at_1</th>\n",
       "      <th>p_at_2</th>\n",
       "      <th>r_at_2</th>\n",
       "      <th>p_at_5</th>\n",
       "      <th>r_at_5</th>\n",
       "      <th>p_at_10</th>\n",
       "      <th>r_at_10</th>\n",
       "      <th>p_at_20</th>\n",
       "      <th>r_at_20</th>\n",
       "      <th>p_at_30</th>\n",
       "      <th>r_at_30</th>\n",
       "      <th>p_at_50</th>\n",
       "      <th>r_at_50</th>\n",
       "      <th>auc-roc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l1'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>0.694881</td>\n",
       "      <td>0.019352</td>\n",
       "      <td>0.877989</td>\n",
       "      <td>0.061146</td>\n",
       "      <td>0.833401</td>\n",
       "      <td>0.116081</td>\n",
       "      <td>0.815211</td>\n",
       "      <td>0.227095</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.782331</td>\n",
       "      <td>0.544850</td>\n",
       "      <td>0.592306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l2'}</td>\n",
       "      <td>0.872211</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>0.880892</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.875963</td>\n",
       "      <td>0.061005</td>\n",
       "      <td>0.857925</td>\n",
       "      <td>0.119497</td>\n",
       "      <td>0.832489</td>\n",
       "      <td>0.231908</td>\n",
       "      <td>0.819247</td>\n",
       "      <td>0.342329</td>\n",
       "      <td>0.788553</td>\n",
       "      <td>0.549183</td>\n",
       "      <td>0.618635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>0.864097</td>\n",
       "      <td>0.012026</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.024137</td>\n",
       "      <td>0.872112</td>\n",
       "      <td>0.060737</td>\n",
       "      <td>0.858533</td>\n",
       "      <td>0.119582</td>\n",
       "      <td>0.837606</td>\n",
       "      <td>0.233334</td>\n",
       "      <td>0.823368</td>\n",
       "      <td>0.344051</td>\n",
       "      <td>0.791573</td>\n",
       "      <td>0.551287</td>\n",
       "      <td>0.624384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.012280</td>\n",
       "      <td>0.872783</td>\n",
       "      <td>0.024306</td>\n",
       "      <td>0.871301</td>\n",
       "      <td>0.060680</td>\n",
       "      <td>0.859141</td>\n",
       "      <td>0.119666</td>\n",
       "      <td>0.838316</td>\n",
       "      <td>0.233531</td>\n",
       "      <td>0.823706</td>\n",
       "      <td>0.344192</td>\n",
       "      <td>0.791999</td>\n",
       "      <td>0.551583</td>\n",
       "      <td>0.625046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>0.834686</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.855043</td>\n",
       "      <td>0.023812</td>\n",
       "      <td>0.869274</td>\n",
       "      <td>0.060539</td>\n",
       "      <td>0.858330</td>\n",
       "      <td>0.119553</td>\n",
       "      <td>0.838164</td>\n",
       "      <td>0.233489</td>\n",
       "      <td>0.823368</td>\n",
       "      <td>0.344051</td>\n",
       "      <td>0.791695</td>\n",
       "      <td>0.551371</td>\n",
       "      <td>0.624894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.839757</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>0.856057</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>0.868869</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.857925</td>\n",
       "      <td>0.119497</td>\n",
       "      <td>0.837809</td>\n",
       "      <td>0.233390</td>\n",
       "      <td>0.823402</td>\n",
       "      <td>0.344065</td>\n",
       "      <td>0.791289</td>\n",
       "      <td>0.551089</td>\n",
       "      <td>0.624866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>0.830629</td>\n",
       "      <td>0.011560</td>\n",
       "      <td>0.854536</td>\n",
       "      <td>0.023798</td>\n",
       "      <td>0.869072</td>\n",
       "      <td>0.060525</td>\n",
       "      <td>0.858026</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>0.838062</td>\n",
       "      <td>0.233461</td>\n",
       "      <td>0.823335</td>\n",
       "      <td>0.344037</td>\n",
       "      <td>0.791451</td>\n",
       "      <td>0.551202</td>\n",
       "      <td>0.624867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.833671</td>\n",
       "      <td>0.011602</td>\n",
       "      <td>0.855043</td>\n",
       "      <td>0.023812</td>\n",
       "      <td>0.868869</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.857722</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.837961</td>\n",
       "      <td>0.233433</td>\n",
       "      <td>0.823368</td>\n",
       "      <td>0.344051</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.551174</td>\n",
       "      <td>0.624857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'min_sam...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.711796</td>\n",
       "      <td>0.099143</td>\n",
       "      <td>0.701409</td>\n",
       "      <td>0.195393</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.613265</td>\n",
       "      <td>0.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'min_sam...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.711796</td>\n",
       "      <td>0.099143</td>\n",
       "      <td>0.701409</td>\n",
       "      <td>0.195393</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.613265</td>\n",
       "      <td>0.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>0.887424</td>\n",
       "      <td>0.012351</td>\n",
       "      <td>0.811455</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.837657</td>\n",
       "      <td>0.058337</td>\n",
       "      <td>0.787191</td>\n",
       "      <td>0.109645</td>\n",
       "      <td>0.799301</td>\n",
       "      <td>0.222663</td>\n",
       "      <td>0.769524</td>\n",
       "      <td>0.321552</td>\n",
       "      <td>0.783993</td>\n",
       "      <td>0.546008</td>\n",
       "      <td>0.610375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>0.887424</td>\n",
       "      <td>0.012351</td>\n",
       "      <td>0.811455</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.837657</td>\n",
       "      <td>0.058337</td>\n",
       "      <td>0.787191</td>\n",
       "      <td>0.109645</td>\n",
       "      <td>0.799301</td>\n",
       "      <td>0.222663</td>\n",
       "      <td>0.769524</td>\n",
       "      <td>0.321552</td>\n",
       "      <td>0.783993</td>\n",
       "      <td>0.546008</td>\n",
       "      <td>0.610375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>0.781947</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.871769</td>\n",
       "      <td>0.024278</td>\n",
       "      <td>0.877989</td>\n",
       "      <td>0.061146</td>\n",
       "      <td>0.847994</td>\n",
       "      <td>0.118114</td>\n",
       "      <td>0.830563</td>\n",
       "      <td>0.231372</td>\n",
       "      <td>0.823808</td>\n",
       "      <td>0.344235</td>\n",
       "      <td>0.792546</td>\n",
       "      <td>0.551964</td>\n",
       "      <td>0.626015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>0.790061</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.872276</td>\n",
       "      <td>0.024292</td>\n",
       "      <td>0.875557</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.847791</td>\n",
       "      <td>0.118085</td>\n",
       "      <td>0.829905</td>\n",
       "      <td>0.231188</td>\n",
       "      <td>0.823639</td>\n",
       "      <td>0.344164</td>\n",
       "      <td>0.792323</td>\n",
       "      <td>0.551809</td>\n",
       "      <td>0.625719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_sa...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139286</td>\n",
       "      <td>0.761603</td>\n",
       "      <td>0.212161</td>\n",
       "      <td>0.841069</td>\n",
       "      <td>0.351447</td>\n",
       "      <td>0.869277</td>\n",
       "      <td>0.605403</td>\n",
       "      <td>0.745723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_sa...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.910113</td>\n",
       "      <td>0.126766</td>\n",
       "      <td>0.784607</td>\n",
       "      <td>0.218570</td>\n",
       "      <td>0.856405</td>\n",
       "      <td>0.357856</td>\n",
       "      <td>0.863319</td>\n",
       "      <td>0.601253</td>\n",
       "      <td>0.734040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_s...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139286</td>\n",
       "      <td>0.744224</td>\n",
       "      <td>0.207320</td>\n",
       "      <td>0.829483</td>\n",
       "      <td>0.346606</td>\n",
       "      <td>0.874040</td>\n",
       "      <td>0.608720</td>\n",
       "      <td>0.748978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_s...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.962910</td>\n",
       "      <td>0.134120</td>\n",
       "      <td>0.769102</td>\n",
       "      <td>0.214250</td>\n",
       "      <td>0.846068</td>\n",
       "      <td>0.353536</td>\n",
       "      <td>0.865224</td>\n",
       "      <td>0.602580</td>\n",
       "      <td>0.736847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'min_...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.711796</td>\n",
       "      <td>0.099143</td>\n",
       "      <td>0.701409</td>\n",
       "      <td>0.195393</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.613265</td>\n",
       "      <td>0.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'min_...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.711796</td>\n",
       "      <td>0.099143</td>\n",
       "      <td>0.701409</td>\n",
       "      <td>0.195393</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.613265</td>\n",
       "      <td>0.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>0.834686</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.783578</td>\n",
       "      <td>0.021822</td>\n",
       "      <td>0.878192</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>0.807458</td>\n",
       "      <td>0.112468</td>\n",
       "      <td>0.810752</td>\n",
       "      <td>0.225853</td>\n",
       "      <td>0.767261</td>\n",
       "      <td>0.320606</td>\n",
       "      <td>0.782027</td>\n",
       "      <td>0.544638</td>\n",
       "      <td>0.609640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>0.834686</td>\n",
       "      <td>0.011617</td>\n",
       "      <td>0.782565</td>\n",
       "      <td>0.021793</td>\n",
       "      <td>0.878395</td>\n",
       "      <td>0.061174</td>\n",
       "      <td>0.807560</td>\n",
       "      <td>0.112482</td>\n",
       "      <td>0.810651</td>\n",
       "      <td>0.225825</td>\n",
       "      <td>0.767295</td>\n",
       "      <td>0.320620</td>\n",
       "      <td>0.781987</td>\n",
       "      <td>0.544610</td>\n",
       "      <td>0.609648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>0.879372</td>\n",
       "      <td>0.024489</td>\n",
       "      <td>0.884070</td>\n",
       "      <td>0.061569</td>\n",
       "      <td>0.858127</td>\n",
       "      <td>0.119525</td>\n",
       "      <td>0.842623</td>\n",
       "      <td>0.234731</td>\n",
       "      <td>0.823166</td>\n",
       "      <td>0.343967</td>\n",
       "      <td>0.794917</td>\n",
       "      <td>0.553616</td>\n",
       "      <td>0.625778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>0.755578</td>\n",
       "      <td>0.010516</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.883462</td>\n",
       "      <td>0.061527</td>\n",
       "      <td>0.857823</td>\n",
       "      <td>0.119483</td>\n",
       "      <td>0.842673</td>\n",
       "      <td>0.234745</td>\n",
       "      <td>0.822828</td>\n",
       "      <td>0.343825</td>\n",
       "      <td>0.794795</td>\n",
       "      <td>0.553531</td>\n",
       "      <td>0.625747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.976794</td>\n",
       "      <td>0.136054</td>\n",
       "      <td>0.768950</td>\n",
       "      <td>0.214208</td>\n",
       "      <td>0.845967</td>\n",
       "      <td>0.353494</td>\n",
       "      <td>0.867777</td>\n",
       "      <td>0.604359</td>\n",
       "      <td>0.742237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.900385</td>\n",
       "      <td>0.125411</td>\n",
       "      <td>0.790231</td>\n",
       "      <td>0.220136</td>\n",
       "      <td>0.860154</td>\n",
       "      <td>0.359422</td>\n",
       "      <td>0.870067</td>\n",
       "      <td>0.605954</td>\n",
       "      <td>0.733306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'mi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139286</td>\n",
       "      <td>0.747669</td>\n",
       "      <td>0.208280</td>\n",
       "      <td>0.831779</td>\n",
       "      <td>0.347566</td>\n",
       "      <td>0.875621</td>\n",
       "      <td>0.609821</td>\n",
       "      <td>0.749111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'mi...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.969193</td>\n",
       "      <td>0.134995</td>\n",
       "      <td>0.770774</td>\n",
       "      <td>0.214716</td>\n",
       "      <td>0.847183</td>\n",
       "      <td>0.354002</td>\n",
       "      <td>0.866825</td>\n",
       "      <td>0.603695</td>\n",
       "      <td>0.739574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l1'}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>0.694881</td>\n",
       "      <td>0.019352</td>\n",
       "      <td>0.877989</td>\n",
       "      <td>0.061146</td>\n",
       "      <td>0.833401</td>\n",
       "      <td>0.116081</td>\n",
       "      <td>0.815211</td>\n",
       "      <td>0.227095</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.782331</td>\n",
       "      <td>0.544850</td>\n",
       "      <td>0.592306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l2'}</td>\n",
       "      <td>0.872211</td>\n",
       "      <td>0.012139</td>\n",
       "      <td>0.880892</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.875963</td>\n",
       "      <td>0.061005</td>\n",
       "      <td>0.857925</td>\n",
       "      <td>0.119497</td>\n",
       "      <td>0.832489</td>\n",
       "      <td>0.231908</td>\n",
       "      <td>0.819247</td>\n",
       "      <td>0.342329</td>\n",
       "      <td>0.788553</td>\n",
       "      <td>0.549183</td>\n",
       "      <td>0.618635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>0.839757</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>0.856057</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>0.868869</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.857925</td>\n",
       "      <td>0.119497</td>\n",
       "      <td>0.837809</td>\n",
       "      <td>0.233390</td>\n",
       "      <td>0.823402</td>\n",
       "      <td>0.344065</td>\n",
       "      <td>0.791289</td>\n",
       "      <td>0.551089</td>\n",
       "      <td>0.624866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>0.830629</td>\n",
       "      <td>0.011560</td>\n",
       "      <td>0.854536</td>\n",
       "      <td>0.023798</td>\n",
       "      <td>0.869072</td>\n",
       "      <td>0.060525</td>\n",
       "      <td>0.858026</td>\n",
       "      <td>0.119511</td>\n",
       "      <td>0.838012</td>\n",
       "      <td>0.233447</td>\n",
       "      <td>0.823301</td>\n",
       "      <td>0.344023</td>\n",
       "      <td>0.791431</td>\n",
       "      <td>0.551188</td>\n",
       "      <td>0.624868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>0.833671</td>\n",
       "      <td>0.011602</td>\n",
       "      <td>0.855043</td>\n",
       "      <td>0.023812</td>\n",
       "      <td>0.868869</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.857722</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.837961</td>\n",
       "      <td>0.233433</td>\n",
       "      <td>0.823368</td>\n",
       "      <td>0.344051</td>\n",
       "      <td>0.791411</td>\n",
       "      <td>0.551174</td>\n",
       "      <td>0.624857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 1}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.711796</td>\n",
       "      <td>0.099143</td>\n",
       "      <td>0.701409</td>\n",
       "      <td>0.195393</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.613265</td>\n",
       "      <td>0.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 10}</td>\n",
       "      <td>0.808316</td>\n",
       "      <td>0.011250</td>\n",
       "      <td>0.874303</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.897244</td>\n",
       "      <td>0.062487</td>\n",
       "      <td>0.826915</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.227843</td>\n",
       "      <td>0.793812</td>\n",
       "      <td>0.331701</td>\n",
       "      <td>0.797937</td>\n",
       "      <td>0.555719</td>\n",
       "      <td>0.603653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 100}</td>\n",
       "      <td>0.826572</td>\n",
       "      <td>0.011504</td>\n",
       "      <td>0.842879</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>0.866234</td>\n",
       "      <td>0.060327</td>\n",
       "      <td>0.847385</td>\n",
       "      <td>0.118029</td>\n",
       "      <td>0.820582</td>\n",
       "      <td>0.228591</td>\n",
       "      <td>0.791177</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.784783</td>\n",
       "      <td>0.546558</td>\n",
       "      <td>0.606189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 1}</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069643</td>\n",
       "      <td>0.711796</td>\n",
       "      <td>0.099143</td>\n",
       "      <td>0.701409</td>\n",
       "      <td>0.195393</td>\n",
       "      <td>0.800939</td>\n",
       "      <td>0.334679</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.613265</td>\n",
       "      <td>0.550781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 10}</td>\n",
       "      <td>0.824544</td>\n",
       "      <td>0.011475</td>\n",
       "      <td>0.829194</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>0.847588</td>\n",
       "      <td>0.059029</td>\n",
       "      <td>0.835124</td>\n",
       "      <td>0.116321</td>\n",
       "      <td>0.825750</td>\n",
       "      <td>0.230031</td>\n",
       "      <td>0.812492</td>\n",
       "      <td>0.339506</td>\n",
       "      <td>0.803976</td>\n",
       "      <td>0.559925</td>\n",
       "      <td>0.612352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 100}</td>\n",
       "      <td>0.831643</td>\n",
       "      <td>0.011574</td>\n",
       "      <td>0.855550</td>\n",
       "      <td>0.023826</td>\n",
       "      <td>0.870288</td>\n",
       "      <td>0.060609</td>\n",
       "      <td>0.859141</td>\n",
       "      <td>0.119666</td>\n",
       "      <td>0.838265</td>\n",
       "      <td>0.233517</td>\n",
       "      <td>0.823706</td>\n",
       "      <td>0.344192</td>\n",
       "      <td>0.792080</td>\n",
       "      <td>0.551639</td>\n",
       "      <td>0.624863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.892495</td>\n",
       "      <td>0.012421</td>\n",
       "      <td>0.884440</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.867653</td>\n",
       "      <td>0.060426</td>\n",
       "      <td>0.857013</td>\n",
       "      <td>0.119370</td>\n",
       "      <td>0.831982</td>\n",
       "      <td>0.231767</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>0.337587</td>\n",
       "      <td>0.782392</td>\n",
       "      <td>0.544893</td>\n",
       "      <td>0.611993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.909736</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>0.888495</td>\n",
       "      <td>0.024743</td>\n",
       "      <td>0.884272</td>\n",
       "      <td>0.061583</td>\n",
       "      <td>0.864613</td>\n",
       "      <td>0.120429</td>\n",
       "      <td>0.832286</td>\n",
       "      <td>0.231852</td>\n",
       "      <td>0.820801</td>\n",
       "      <td>0.342979</td>\n",
       "      <td>0.787945</td>\n",
       "      <td>0.548760</td>\n",
       "      <td>0.621082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.895590</td>\n",
       "      <td>0.024941</td>\n",
       "      <td>0.872923</td>\n",
       "      <td>0.060793</td>\n",
       "      <td>0.859242</td>\n",
       "      <td>0.119680</td>\n",
       "      <td>0.828739</td>\n",
       "      <td>0.230864</td>\n",
       "      <td>0.804114</td>\n",
       "      <td>0.336006</td>\n",
       "      <td>0.781541</td>\n",
       "      <td>0.544300</td>\n",
       "      <td>0.610145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>0.905680</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.889002</td>\n",
       "      <td>0.024758</td>\n",
       "      <td>0.886299</td>\n",
       "      <td>0.061725</td>\n",
       "      <td>0.864208</td>\n",
       "      <td>0.120372</td>\n",
       "      <td>0.831779</td>\n",
       "      <td>0.231711</td>\n",
       "      <td>0.819112</td>\n",
       "      <td>0.342273</td>\n",
       "      <td>0.787520</td>\n",
       "      <td>0.548464</td>\n",
       "      <td>0.620299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>0.889452</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.878865</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.867248</td>\n",
       "      <td>0.060398</td>\n",
       "      <td>0.856810</td>\n",
       "      <td>0.119342</td>\n",
       "      <td>0.830310</td>\n",
       "      <td>0.231301</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.341892</td>\n",
       "      <td>0.786324</td>\n",
       "      <td>0.547631</td>\n",
       "      <td>0.616561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>0.894523</td>\n",
       "      <td>0.012449</td>\n",
       "      <td>0.894577</td>\n",
       "      <td>0.024913</td>\n",
       "      <td>0.883259</td>\n",
       "      <td>0.061513</td>\n",
       "      <td>0.861674</td>\n",
       "      <td>0.120019</td>\n",
       "      <td>0.836137</td>\n",
       "      <td>0.232924</td>\n",
       "      <td>0.822524</td>\n",
       "      <td>0.343698</td>\n",
       "      <td>0.789019</td>\n",
       "      <td>0.549508</td>\n",
       "      <td>0.622608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>0.895538</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>0.881399</td>\n",
       "      <td>0.024546</td>\n",
       "      <td>0.875557</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.855188</td>\n",
       "      <td>0.119116</td>\n",
       "      <td>0.830006</td>\n",
       "      <td>0.231217</td>\n",
       "      <td>0.819112</td>\n",
       "      <td>0.342273</td>\n",
       "      <td>0.787418</td>\n",
       "      <td>0.548393</td>\n",
       "      <td>0.617727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>0.897566</td>\n",
       "      <td>0.012492</td>\n",
       "      <td>0.891536</td>\n",
       "      <td>0.024828</td>\n",
       "      <td>0.881638</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.863498</td>\n",
       "      <td>0.120273</td>\n",
       "      <td>0.836796</td>\n",
       "      <td>0.233108</td>\n",
       "      <td>0.823909</td>\n",
       "      <td>0.344277</td>\n",
       "      <td>0.789810</td>\n",
       "      <td>0.550059</td>\n",
       "      <td>0.623095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>0.838670</td>\n",
       "      <td>0.058408</td>\n",
       "      <td>0.919335</td>\n",
       "      <td>0.128051</td>\n",
       "      <td>0.925618</td>\n",
       "      <td>0.257851</td>\n",
       "      <td>0.911127</td>\n",
       "      <td>0.380722</td>\n",
       "      <td>0.871770</td>\n",
       "      <td>0.607139</td>\n",
       "      <td>0.764817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>0.952333</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.954384</td>\n",
       "      <td>0.026578</td>\n",
       "      <td>0.954601</td>\n",
       "      <td>0.066481</td>\n",
       "      <td>0.949027</td>\n",
       "      <td>0.132186</td>\n",
       "      <td>0.933877</td>\n",
       "      <td>0.260152</td>\n",
       "      <td>0.918322</td>\n",
       "      <td>0.383728</td>\n",
       "      <td>0.880627</td>\n",
       "      <td>0.613308</td>\n",
       "      <td>0.779924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>0.953347</td>\n",
       "      <td>0.013268</td>\n",
       "      <td>0.945261</td>\n",
       "      <td>0.026324</td>\n",
       "      <td>0.943859</td>\n",
       "      <td>0.065733</td>\n",
       "      <td>0.932509</td>\n",
       "      <td>0.129886</td>\n",
       "      <td>0.911532</td>\n",
       "      <td>0.253927</td>\n",
       "      <td>0.894643</td>\n",
       "      <td>0.373834</td>\n",
       "      <td>0.854219</td>\n",
       "      <td>0.594916</td>\n",
       "      <td>0.733735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>0.959432</td>\n",
       "      <td>0.013353</td>\n",
       "      <td>0.955398</td>\n",
       "      <td>0.026607</td>\n",
       "      <td>0.949737</td>\n",
       "      <td>0.066143</td>\n",
       "      <td>0.940717</td>\n",
       "      <td>0.131029</td>\n",
       "      <td>0.919437</td>\n",
       "      <td>0.256129</td>\n",
       "      <td>0.903831</td>\n",
       "      <td>0.377673</td>\n",
       "      <td>0.866602</td>\n",
       "      <td>0.603540</td>\n",
       "      <td>0.754929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>0.846777</td>\n",
       "      <td>0.058972</td>\n",
       "      <td>0.923389</td>\n",
       "      <td>0.128615</td>\n",
       "      <td>0.925517</td>\n",
       "      <td>0.257823</td>\n",
       "      <td>0.910147</td>\n",
       "      <td>0.380313</td>\n",
       "      <td>0.870898</td>\n",
       "      <td>0.606532</td>\n",
       "      <td>0.763718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>0.968560</td>\n",
       "      <td>0.013480</td>\n",
       "      <td>0.953371</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>0.952979</td>\n",
       "      <td>0.066368</td>\n",
       "      <td>0.947102</td>\n",
       "      <td>0.131918</td>\n",
       "      <td>0.936056</td>\n",
       "      <td>0.260759</td>\n",
       "      <td>0.920754</td>\n",
       "      <td>0.384745</td>\n",
       "      <td>0.879654</td>\n",
       "      <td>0.612630</td>\n",
       "      <td>0.780529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>0.954361</td>\n",
       "      <td>0.013282</td>\n",
       "      <td>0.953877</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.942440</td>\n",
       "      <td>0.065634</td>\n",
       "      <td>0.931901</td>\n",
       "      <td>0.129801</td>\n",
       "      <td>0.911735</td>\n",
       "      <td>0.253984</td>\n",
       "      <td>0.894541</td>\n",
       "      <td>0.373791</td>\n",
       "      <td>0.855191</td>\n",
       "      <td>0.595593</td>\n",
       "      <td>0.734312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>0.958418</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.956918</td>\n",
       "      <td>0.026649</td>\n",
       "      <td>0.951155</td>\n",
       "      <td>0.066241</td>\n",
       "      <td>0.940312</td>\n",
       "      <td>0.130972</td>\n",
       "      <td>0.922173</td>\n",
       "      <td>0.256892</td>\n",
       "      <td>0.903223</td>\n",
       "      <td>0.377419</td>\n",
       "      <td>0.867473</td>\n",
       "      <td>0.604147</td>\n",
       "      <td>0.755189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SVM</td>\n",
       "      <td>LinearSVC(C=10, class_weight=None, dual=True, ...</td>\n",
       "      <td>{'C': 0.001}</td>\n",
       "      <td>0.891481</td>\n",
       "      <td>0.012407</td>\n",
       "      <td>0.892043</td>\n",
       "      <td>0.024842</td>\n",
       "      <td>0.880219</td>\n",
       "      <td>0.061301</td>\n",
       "      <td>0.863296</td>\n",
       "      <td>0.120245</td>\n",
       "      <td>0.839380</td>\n",
       "      <td>0.233828</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.344136</td>\n",
       "      <td>0.791999</td>\n",
       "      <td>0.551583</td>\n",
       "      <td>0.625333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>SVM</td>\n",
       "      <td>LinearSVC(C=10, class_weight=None, dual=True, ...</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>0.885396</td>\n",
       "      <td>0.012322</td>\n",
       "      <td>0.887988</td>\n",
       "      <td>0.024729</td>\n",
       "      <td>0.876571</td>\n",
       "      <td>0.061047</td>\n",
       "      <td>0.860053</td>\n",
       "      <td>0.119793</td>\n",
       "      <td>0.838670</td>\n",
       "      <td>0.233630</td>\n",
       "      <td>0.824078</td>\n",
       "      <td>0.344348</td>\n",
       "      <td>0.791897</td>\n",
       "      <td>0.551512</td>\n",
       "      <td>0.625729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>SVM</td>\n",
       "      <td>LinearSVC(C=10, class_weight=None, dual=True, ...</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.880325</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.880385</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>0.872720</td>\n",
       "      <td>0.060779</td>\n",
       "      <td>0.858938</td>\n",
       "      <td>0.119638</td>\n",
       "      <td>0.837708</td>\n",
       "      <td>0.233362</td>\n",
       "      <td>0.823639</td>\n",
       "      <td>0.344164</td>\n",
       "      <td>0.792140</td>\n",
       "      <td>0.551682</td>\n",
       "      <td>0.625592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>SVM</td>\n",
       "      <td>LinearSVC(C=10, class_weight=None, dual=True, ...</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>0.884381</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.872112</td>\n",
       "      <td>0.060737</td>\n",
       "      <td>0.859141</td>\n",
       "      <td>0.119666</td>\n",
       "      <td>0.837708</td>\n",
       "      <td>0.233362</td>\n",
       "      <td>0.823706</td>\n",
       "      <td>0.344192</td>\n",
       "      <td>0.791978</td>\n",
       "      <td>0.551569</td>\n",
       "      <td>0.625581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>SVM</td>\n",
       "      <td>LinearSVC(C=10, class_weight=None, dual=True, ...</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.884381</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.875824</td>\n",
       "      <td>0.024391</td>\n",
       "      <td>0.875557</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.857722</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.837404</td>\n",
       "      <td>0.233277</td>\n",
       "      <td>0.823672</td>\n",
       "      <td>0.344178</td>\n",
       "      <td>0.791613</td>\n",
       "      <td>0.551315</td>\n",
       "      <td>0.625121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name                                              model  \\\n",
       "0          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "1          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "2          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "3          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "4          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "5          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "6          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "7          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "8          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "9          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "10         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "11         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "12         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "13         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "14         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "15         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "16         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "17         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "18         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "19         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "20         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "21         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "22         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "23         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "24         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "25         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "26         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "27         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "28         LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "29         LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "..        ...                                                ...   \n",
       "33         LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "34         LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "35         LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "36         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "37         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "38         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "39         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "40         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "41         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "42         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "43         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "44         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "45         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "46         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "47         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "48         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "49         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "50         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "51         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "52         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "53         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "54         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "55         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "56         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "57         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "58        SVM  LinearSVC(C=10, class_weight=None, dual=True, ...   \n",
       "59        SVM  LinearSVC(C=10, class_weight=None, dual=True, ...   \n",
       "60        SVM  LinearSVC(C=10, class_weight=None, dual=True, ...   \n",
       "61        SVM  LinearSVC(C=10, class_weight=None, dual=True, ...   \n",
       "62        SVM  LinearSVC(C=10, class_weight=None, dual=True, ...   \n",
       "\n",
       "                                           parameters    p_at_1    r_at_1  \\\n",
       "0                       {'C': 0.001, 'penalty': 'l1'}  1.000000  0.013917   \n",
       "1                       {'C': 0.001, 'penalty': 'l2'}  0.872211  0.012139   \n",
       "2                         {'C': 0.1, 'penalty': 'l1'}  0.864097  0.012026   \n",
       "3                         {'C': 0.1, 'penalty': 'l2'}  0.882353  0.012280   \n",
       "4                           {'C': 1, 'penalty': 'l1'}  0.834686  0.011617   \n",
       "5                           {'C': 1, 'penalty': 'l2'}  0.839757  0.011687   \n",
       "6                          {'C': 10, 'penalty': 'l1'}  0.830629  0.011560   \n",
       "7                          {'C': 10, 'penalty': 'l2'}  0.833671  0.011602   \n",
       "8   {'criterion': 'gini', 'max_depth': 1, 'min_sam...  1.000000  0.013917   \n",
       "9   {'criterion': 'gini', 'max_depth': 1, 'min_sam...  1.000000  0.013917   \n",
       "10  {'criterion': 'gini', 'max_depth': 5, 'min_sam...  0.887424  0.012351   \n",
       "11  {'criterion': 'gini', 'max_depth': 5, 'min_sam...  0.887424  0.012351   \n",
       "12  {'criterion': 'gini', 'max_depth': 10, 'min_sa...  0.781947  0.010883   \n",
       "13  {'criterion': 'gini', 'max_depth': 10, 'min_sa...  0.790061  0.010996   \n",
       "14  {'criterion': 'gini', 'max_depth': 50, 'min_sa...  1.000000  0.013917   \n",
       "15  {'criterion': 'gini', 'max_depth': 50, 'min_sa...  1.000000  0.013917   \n",
       "16  {'criterion': 'gini', 'max_depth': 100, 'min_s...  1.000000  0.013917   \n",
       "17  {'criterion': 'gini', 'max_depth': 100, 'min_s...  1.000000  0.013917   \n",
       "18  {'criterion': 'entropy', 'max_depth': 1, 'min_...  1.000000  0.013917   \n",
       "19  {'criterion': 'entropy', 'max_depth': 1, 'min_...  1.000000  0.013917   \n",
       "20  {'criterion': 'entropy', 'max_depth': 5, 'min_...  0.834686  0.011617   \n",
       "21  {'criterion': 'entropy', 'max_depth': 5, 'min_...  0.834686  0.011617   \n",
       "22  {'criterion': 'entropy', 'max_depth': 10, 'min...  0.758621  0.010558   \n",
       "23  {'criterion': 'entropy', 'max_depth': 10, 'min...  0.755578  0.010516   \n",
       "24  {'criterion': 'entropy', 'max_depth': 50, 'min...  1.000000  0.013917   \n",
       "25  {'criterion': 'entropy', 'max_depth': 50, 'min...  1.000000  0.013917   \n",
       "26  {'criterion': 'entropy', 'max_depth': 100, 'mi...  1.000000  0.013917   \n",
       "27  {'criterion': 'entropy', 'max_depth': 100, 'mi...  1.000000  0.013917   \n",
       "28                      {'C': 0.001, 'penalty': 'l1'}  1.000000  0.013917   \n",
       "29                      {'C': 0.001, 'penalty': 'l2'}  0.872211  0.012139   \n",
       "..                                                ...       ...       ...   \n",
       "33                          {'C': 1, 'penalty': 'l2'}  0.839757  0.011687   \n",
       "34                         {'C': 10, 'penalty': 'l1'}  0.830629  0.011560   \n",
       "35                         {'C': 10, 'penalty': 'l2'}  0.833671  0.011602   \n",
       "36          {'algorithm': 'SAMME', 'n_estimators': 1}  1.000000  0.013917   \n",
       "37         {'algorithm': 'SAMME', 'n_estimators': 10}  0.808316  0.011250   \n",
       "38        {'algorithm': 'SAMME', 'n_estimators': 100}  0.826572  0.011504   \n",
       "39        {'algorithm': 'SAMME.R', 'n_estimators': 1}  1.000000  0.013917   \n",
       "40       {'algorithm': 'SAMME.R', 'n_estimators': 10}  0.824544  0.011475   \n",
       "41      {'algorithm': 'SAMME.R', 'n_estimators': 100}  0.831643  0.011574   \n",
       "42  {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.892495  0.012421   \n",
       "43  {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.909736  0.012661   \n",
       "44  {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.911765  0.012689   \n",
       "45  {'max_depth': 5, 'max_features': 'sqrt', 'min_...  0.905680  0.012605   \n",
       "46  {'max_depth': 5, 'max_features': 'log2', 'min_...  0.889452  0.012379   \n",
       "47  {'max_depth': 5, 'max_features': 'log2', 'min_...  0.894523  0.012449   \n",
       "48  {'max_depth': 5, 'max_features': 'log2', 'min_...  0.895538  0.012463   \n",
       "49  {'max_depth': 5, 'max_features': 'log2', 'min_...  0.897566  0.012492   \n",
       "50  {'max_depth': 50, 'max_features': 'sqrt', 'min...  1.000000  0.013917   \n",
       "51  {'max_depth': 50, 'max_features': 'sqrt', 'min...  0.952333  0.013254   \n",
       "52  {'max_depth': 50, 'max_features': 'sqrt', 'min...  0.953347  0.013268   \n",
       "53  {'max_depth': 50, 'max_features': 'sqrt', 'min...  0.959432  0.013353   \n",
       "54  {'max_depth': 50, 'max_features': 'log2', 'min...  1.000000  0.013917   \n",
       "55  {'max_depth': 50, 'max_features': 'log2', 'min...  0.968560  0.013480   \n",
       "56  {'max_depth': 50, 'max_features': 'log2', 'min...  0.954361  0.013282   \n",
       "57  {'max_depth': 50, 'max_features': 'log2', 'min...  0.958418  0.013339   \n",
       "58                                       {'C': 0.001}  0.891481  0.012407   \n",
       "59                                        {'C': 0.01}  0.885396  0.012322   \n",
       "60                                         {'C': 0.1}  0.880325  0.012252   \n",
       "61                                           {'C': 1}  0.884381  0.012308   \n",
       "62                                          {'C': 10}  0.884381  0.012308   \n",
       "\n",
       "      p_at_2    r_at_2    p_at_5    r_at_5   p_at_10   r_at_10   p_at_20  \\\n",
       "0   0.694881  0.019352  0.877989  0.061146  0.833401  0.116081  0.815211   \n",
       "1   0.880892  0.024532  0.875963  0.061005  0.857925  0.119497  0.832489   \n",
       "2   0.866700  0.024137  0.872112  0.060737  0.858533  0.119582  0.837606   \n",
       "3   0.872783  0.024306  0.871301  0.060680  0.859141  0.119666  0.838316   \n",
       "4   0.855043  0.023812  0.869274  0.060539  0.858330  0.119553  0.838164   \n",
       "5   0.856057  0.023840  0.868869  0.060511  0.857925  0.119497  0.837809   \n",
       "6   0.854536  0.023798  0.869072  0.060525  0.858026  0.119511  0.838062   \n",
       "7   0.855043  0.023812  0.868869  0.060511  0.857722  0.119469  0.837961   \n",
       "8   1.000000  0.027849  1.000000  0.069643  0.711796  0.099143  0.701409   \n",
       "9   1.000000  0.027849  1.000000  0.069643  0.711796  0.099143  0.701409   \n",
       "10  0.811455  0.022598  0.837657  0.058337  0.787191  0.109645  0.799301   \n",
       "11  0.811455  0.022598  0.837657  0.058337  0.787191  0.109645  0.799301   \n",
       "12  0.871769  0.024278  0.877989  0.061146  0.847994  0.118114  0.830563   \n",
       "13  0.872276  0.024292  0.875557  0.060976  0.847791  0.118085  0.829905   \n",
       "14  1.000000  0.027849  1.000000  0.069643  1.000000  0.139286  0.761603   \n",
       "15  1.000000  0.027849  1.000000  0.069643  0.910113  0.126766  0.784607   \n",
       "16  1.000000  0.027849  1.000000  0.069643  1.000000  0.139286  0.744224   \n",
       "17  1.000000  0.027849  1.000000  0.069643  0.962910  0.134120  0.769102   \n",
       "18  1.000000  0.027849  1.000000  0.069643  0.711796  0.099143  0.701409   \n",
       "19  1.000000  0.027849  1.000000  0.069643  0.711796  0.099143  0.701409   \n",
       "20  0.783578  0.021822  0.878192  0.061160  0.807458  0.112468  0.810752   \n",
       "21  0.782565  0.021793  0.878395  0.061174  0.807560  0.112482  0.810651   \n",
       "22  0.879372  0.024489  0.884070  0.061569  0.858127  0.119525  0.842623   \n",
       "23  0.877851  0.024447  0.883462  0.061527  0.857823  0.119483  0.842673   \n",
       "24  1.000000  0.027849  1.000000  0.069643  0.976794  0.136054  0.768950   \n",
       "25  1.000000  0.027849  1.000000  0.069643  0.900385  0.125411  0.790231   \n",
       "26  1.000000  0.027849  1.000000  0.069643  1.000000  0.139286  0.747669   \n",
       "27  1.000000  0.027849  1.000000  0.069643  0.969193  0.134995  0.770774   \n",
       "28  0.694881  0.019352  0.877989  0.061146  0.833401  0.116081  0.815211   \n",
       "29  0.880892  0.024532  0.875963  0.061005  0.857925  0.119497  0.832489   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "33  0.856057  0.023840  0.868869  0.060511  0.857925  0.119497  0.837809   \n",
       "34  0.854536  0.023798  0.869072  0.060525  0.858026  0.119511  0.838012   \n",
       "35  0.855043  0.023812  0.868869  0.060511  0.857722  0.119469  0.837961   \n",
       "36  1.000000  0.027849  1.000000  0.069643  0.711796  0.099143  0.701409   \n",
       "37  0.874303  0.024348  0.897244  0.062487  0.826915  0.115178  0.817896   \n",
       "38  0.842879  0.023473  0.866234  0.060327  0.847385  0.118029  0.820582   \n",
       "39  1.000000  0.027849  1.000000  0.069643  0.711796  0.099143  0.701409   \n",
       "40  0.829194  0.023092  0.847588  0.059029  0.835124  0.116321  0.825750   \n",
       "41  0.855550  0.023826  0.870288  0.060609  0.859141  0.119666  0.838265   \n",
       "42  0.884440  0.024631  0.867653  0.060426  0.857013  0.119370  0.831982   \n",
       "43  0.888495  0.024743  0.884272  0.061583  0.864613  0.120429  0.832286   \n",
       "44  0.895590  0.024941  0.872923  0.060793  0.859242  0.119680  0.828739   \n",
       "45  0.889002  0.024758  0.886299  0.061725  0.864208  0.120372  0.831779   \n",
       "46  0.878865  0.024475  0.867248  0.060398  0.856810  0.119342  0.830310   \n",
       "47  0.894577  0.024913  0.883259  0.061513  0.861674  0.120019  0.836137   \n",
       "48  0.881399  0.024546  0.875557  0.060976  0.855188  0.119116  0.830006   \n",
       "49  0.891536  0.024828  0.881638  0.061400  0.863498  0.120273  0.836796   \n",
       "50  1.000000  0.027849  0.838670  0.058408  0.919335  0.128051  0.925618   \n",
       "51  0.954384  0.026578  0.954601  0.066481  0.949027  0.132186  0.933877   \n",
       "52  0.945261  0.026324  0.943859  0.065733  0.932509  0.129886  0.911532   \n",
       "53  0.955398  0.026607  0.949737  0.066143  0.940717  0.131029  0.919437   \n",
       "54  1.000000  0.027849  0.846777  0.058972  0.923389  0.128615  0.925517   \n",
       "55  0.953371  0.026550  0.952979  0.066368  0.947102  0.131918  0.936056   \n",
       "56  0.953877  0.026564  0.942440  0.065634  0.931901  0.129801  0.911735   \n",
       "57  0.956918  0.026649  0.951155  0.066241  0.940312  0.130972  0.922173   \n",
       "58  0.892043  0.024842  0.880219  0.061301  0.863296  0.120245  0.839380   \n",
       "59  0.887988  0.024729  0.876571  0.061047  0.860053  0.119793  0.838670   \n",
       "60  0.880385  0.024518  0.872720  0.060779  0.858938  0.119638  0.837708   \n",
       "61  0.877851  0.024447  0.872112  0.060737  0.859141  0.119666  0.837708   \n",
       "62  0.875824  0.024391  0.875557  0.060976  0.857722  0.119469  0.837404   \n",
       "\n",
       "     r_at_20   p_at_30   r_at_30   p_at_50   r_at_50   auc-roc  \n",
       "0   0.227095  0.800939  0.334679  0.782331  0.544850  0.592306  \n",
       "1   0.231908  0.819247  0.342329  0.788553  0.549183  0.618635  \n",
       "2   0.233334  0.823368  0.344051  0.791573  0.551287  0.624384  \n",
       "3   0.233531  0.823706  0.344192  0.791999  0.551583  0.625046  \n",
       "4   0.233489  0.823368  0.344051  0.791695  0.551371  0.624894  \n",
       "5   0.233390  0.823402  0.344065  0.791289  0.551089  0.624866  \n",
       "6   0.233461  0.823335  0.344037  0.791451  0.551202  0.624867  \n",
       "7   0.233433  0.823368  0.344051  0.791411  0.551174  0.624857  \n",
       "8   0.195393  0.800939  0.334679  0.880566  0.613265  0.550781  \n",
       "9   0.195393  0.800939  0.334679  0.880566  0.613265  0.550781  \n",
       "10  0.222663  0.769524  0.321552  0.783993  0.546008  0.610375  \n",
       "11  0.222663  0.769524  0.321552  0.783993  0.546008  0.610375  \n",
       "12  0.231372  0.823808  0.344235  0.792546  0.551964  0.626015  \n",
       "13  0.231188  0.823639  0.344164  0.792323  0.551809  0.625719  \n",
       "14  0.212161  0.841069  0.351447  0.869277  0.605403  0.745723  \n",
       "15  0.218570  0.856405  0.357856  0.863319  0.601253  0.734040  \n",
       "16  0.207320  0.829483  0.346606  0.874040  0.608720  0.748978  \n",
       "17  0.214250  0.846068  0.353536  0.865224  0.602580  0.736847  \n",
       "18  0.195393  0.800939  0.334679  0.880566  0.613265  0.550781  \n",
       "19  0.195393  0.800939  0.334679  0.880566  0.613265  0.550781  \n",
       "20  0.225853  0.767261  0.320606  0.782027  0.544638  0.609640  \n",
       "21  0.225825  0.767295  0.320620  0.781987  0.544610  0.609648  \n",
       "22  0.234731  0.823166  0.343967  0.794917  0.553616  0.625778  \n",
       "23  0.234745  0.822828  0.343825  0.794795  0.553531  0.625747  \n",
       "24  0.214208  0.845967  0.353494  0.867777  0.604359  0.742237  \n",
       "25  0.220136  0.860154  0.359422  0.870067  0.605954  0.733306  \n",
       "26  0.208280  0.831779  0.347566  0.875621  0.609821  0.749111  \n",
       "27  0.214716  0.847183  0.354002  0.866825  0.603695  0.739574  \n",
       "28  0.227095  0.800939  0.334679  0.782331  0.544850  0.592306  \n",
       "29  0.231908  0.819247  0.342329  0.788553  0.549183  0.618635  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "33  0.233390  0.823402  0.344065  0.791289  0.551089  0.624866  \n",
       "34  0.233447  0.823301  0.344023  0.791431  0.551188  0.624868  \n",
       "35  0.233433  0.823368  0.344051  0.791411  0.551174  0.624857  \n",
       "36  0.195393  0.800939  0.334679  0.880566  0.613265  0.550781  \n",
       "37  0.227843  0.793812  0.331701  0.797937  0.555719  0.603653  \n",
       "38  0.228591  0.791177  0.330600  0.784783  0.546558  0.606189  \n",
       "39  0.195393  0.800939  0.334679  0.880566  0.613265  0.550781  \n",
       "40  0.230031  0.812492  0.339506  0.803976  0.559925  0.612352  \n",
       "41  0.233517  0.823706  0.344192  0.792080  0.551639  0.624863  \n",
       "42  0.231767  0.807898  0.337587  0.782392  0.544893  0.611993  \n",
       "43  0.231852  0.820801  0.342979  0.787945  0.548760  0.621082  \n",
       "44  0.230864  0.804114  0.336006  0.781541  0.544300  0.610145  \n",
       "45  0.231711  0.819112  0.342273  0.787520  0.548464  0.620299  \n",
       "46  0.231301  0.818200  0.341892  0.786324  0.547631  0.616561  \n",
       "47  0.232924  0.822524  0.343698  0.789019  0.549508  0.622608  \n",
       "48  0.231217  0.819112  0.342273  0.787418  0.548393  0.617727  \n",
       "49  0.233108  0.823909  0.344277  0.789810  0.550059  0.623095  \n",
       "50  0.257851  0.911127  0.380722  0.871770  0.607139  0.764817  \n",
       "51  0.260152  0.918322  0.383728  0.880627  0.613308  0.779924  \n",
       "52  0.253927  0.894643  0.373834  0.854219  0.594916  0.733735  \n",
       "53  0.256129  0.903831  0.377673  0.866602  0.603540  0.754929  \n",
       "54  0.257823  0.910147  0.380313  0.870898  0.606532  0.763718  \n",
       "55  0.260759  0.920754  0.384745  0.879654  0.612630  0.780529  \n",
       "56  0.253984  0.894541  0.373791  0.855191  0.595593  0.734312  \n",
       "57  0.256892  0.903223  0.377419  0.867473  0.604147  0.755189  \n",
       "58  0.233828  0.823571  0.344136  0.791999  0.551583  0.625333  \n",
       "59  0.233630  0.824078  0.344348  0.791897  0.551512  0.625729  \n",
       "60  0.233362  0.823639  0.344164  0.792140  0.551682  0.625592  \n",
       "61  0.233362  0.823706  0.344192  0.791978  0.551569  0.625581  \n",
       "62  0.233277  0.823672  0.344178  0.791613  0.551315  0.625121  \n",
       "\n",
       "[63 rows x 18 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"With third train set (smallest one):\")\n",
    "pipeline.iterate_over_models(models_to_run, models, parameters_grid, x_train_3, x_test_3, y_train_3, y_test_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
