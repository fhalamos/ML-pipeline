{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired/based on https://github.com/rayidghani/magicloops and https://github.com/dssg/MLforPublicPolicy/blob/master/labs/2019/lab6_feature_generation_sol.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_v2 as pipeline\n",
    "import importlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "datafile = \"projects_2012_2013.csv\"\n",
    "data = pd.read_csv(datafile, parse_dates=['date_posted', 'datefullyfunded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exploration...\n",
      "\n",
      "Number of rows: 124976\n",
      "\n",
      "Columns and types of data:\n",
      "projectid                                         object\n",
      "teacher_acctid                                    object\n",
      "schoolid                                          object\n",
      "school_ncesid                                    float64\n",
      "school_latitude                                  float64\n",
      "school_longitude                                 float64\n",
      "school_city                                       object\n",
      "school_state                                      object\n",
      "school_metro                                      object\n",
      "school_district                                   object\n",
      "school_county                                     object\n",
      "school_charter                                    object\n",
      "school_magnet                                     object\n",
      "teacher_prefix                                    object\n",
      "primary_focus_subject                             object\n",
      "primary_focus_area                                object\n",
      "secondary_focus_subject                           object\n",
      "secondary_focus_area                              object\n",
      "resource_type                                     object\n",
      "poverty_level                                     object\n",
      "grade_level                                       object\n",
      "total_price_including_optional_support           float64\n",
      "students_reached                                 float64\n",
      "eligible_double_your_impact_match                 object\n",
      "date_posted                               datetime64[ns]\n",
      "datefullyfunded                           datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Statistics for selected variables:\n",
      "count    124976.000000\n",
      "mean        654.011811\n",
      "std        1098.015854\n",
      "min          92.000000\n",
      "25%         345.810000\n",
      "50%         510.500000\n",
      "75%         752.960000\n",
      "max      164382.840000\n",
      "Name: total_price_including_optional_support, dtype: float64\n",
      "Number of outliers (>4 standard dev):521\n",
      "\n",
      "\n",
      "count    124917.000000\n",
      "mean         95.445760\n",
      "std         163.481912\n",
      "min           1.000000\n",
      "25%          23.000000\n",
      "50%          30.000000\n",
      "75%         100.000000\n",
      "max       12143.000000\n",
      "Name: students_reached, dtype: float64\n",
      "Number of outliers (>4 standard dev):2063\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Correlation between ['total_price_including_optional_support', 'students_reached']\n",
      "0.1766274350291622\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "stats_for_variables = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]\n",
    "\n",
    "variables_for_correlation = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]\n",
    "\n",
    "pipeline.explore_data(data, stats_for_variables, variables_for_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see in the histograms that total_price_including_optional_support and students_reached have very big outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create outcome label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "# The task is to predict if a project will not get fully funded within 60 days of posting.\n",
    "data['duration_of_funding'] = data.datefullyfunded - data.date_posted\n",
    "data['not_funded_in_60'] =  np.where(data['duration_of_funding']<=pd.Timedelta('60 days'), 1, 0)\n",
    "output_label ='not_funded_in_60'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create binary features for categorical data, discretize features for contiuous data, and an aggregation feature\n",
    "\n",
    "#Select columns from which we will create binary features. We use string string columns who have less than 50 different values (we dont want to generate too many binary values)\n",
    "str_columns = [column for column in data.columns if data[column].dtype=='object' and len(data[column].unique())<51]\n",
    "\n",
    "#Columns with float values to generate discrete features\n",
    "float_columns = ['total_price_including_optional_support', 'students_reached']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#Generate the binary features\n",
    "features = pipeline.create_dummies(data[str_columns], str_columns)\n",
    "\n",
    "#Genereate discretized features. Using qcut due to outliers (if not, almost all datapoints end up in 'low')\n",
    "for float_column in float_columns:\n",
    "  features[float_column] = pd.qcut(data[float_column], 5, labels=['low', 'medium low', 'medium', 'medium high', 'high'])\n",
    "  \n",
    "# Generate binary features for the new discretized columns\n",
    "features = pipeline.create_dummies(features, float_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate feature: number of projects that got funded in the last 10 days. Idea: if many projects have been funded lately, this could be good info to predictor if a project will be funded soon\n",
    "\n",
    "#List of all dates where projects have been posted\n",
    "date_posted_list = pd.to_datetime(data['date_posted'].unique())\n",
    "\n",
    "#We use a dictionary to save the amount of projects that have been funded within the last 10 days in each specific day\n",
    "num_projects_funded_dict = {}\n",
    "\n",
    "#For every possible date_posted\n",
    "for date_posted in date_posted_list:\n",
    "  #For each project, we calculate the difference between the current observed date and the project funded date\n",
    "  #Lets remember that the difference between a value (date_posted) and a series (data['datefullyfunded']) is a series\n",
    "  diff_date_and_fully_funded = date_posted - data['datefullyfunded']\n",
    "  \n",
    "  #Count how many projects have a difference between fully funded date and current date bigger than 0 and smaller or equal than 10\n",
    "  amount_funded_in_last_10_days = np.sum((diff_date_and_fully_funded>pd.Timedelta('0 days')) & (diff_date_and_fully_funded<=pd.Timedelta('10 days')))\n",
    "  \n",
    "  #Save the amount in dictionary\n",
    "  num_projects_funded_dict[date_posted.strftime(\"%Y%m%d\")]= amount_funded_in_last_10_days\n",
    "\n",
    "#We create the column to be attached, initially full of zeros for each row in the dataframe\n",
    "num_of_projects_funded_10_days = np.zeros(len(data))\n",
    "for index, row in data.iterrows():\n",
    "  num_of_projects_funded_10_days[index] = num_projects_funded_dict[row['date_posted'].strftime(\"%Y%m%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach the created column to features\n",
    "features['num_of_projects_funded_10_days']=num_of_projects_funded_10_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create temporal train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#Create three sets of train and test data, based on threee different split thresholds\n",
    "split_thresholds = [pd.Timestamp(2013,7,1), pd.Timestamp(2013,1,1), pd.Timestamp(2012,7,1)]\n",
    "\n",
    "#Indicating which is the column to be used for splitting training and test daata\n",
    "date_column='date_posted'\n",
    "\n",
    "#Amount of data used for test set\n",
    "test_window = relativedelta(months=6)\n",
    "\n",
    "#Gap needed between training and test set. 60 days in this case\n",
    "gap_training_test = relativedelta(days=100)\n",
    "\n",
    "#Generate train and test sets\n",
    "train_test_sets= pipeline.create_temp_validation_train_and_testing_sets(\n",
    "  data,\n",
    "  features,\n",
    "  date_column,\n",
    "  output_label,\n",
    "  split_thresholds,\n",
    "  test_window,\n",
    "  gap_training_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "columns_to_process = [\n",
    "  'total_price_including_optional_support',\n",
    "  'students_reached'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Clasifiers and parameters generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "#We define the specific models we want to run\n",
    "models_to_run=['LR', 'DT', 'AB','RF']#,'KNN']#,'BA','SVM']\n",
    "\n",
    "#Get all posible models and their different sets of parameters\n",
    "models, parameters_grid = pipeline.get_models_and_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over models and different training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LR with params: {'penalty': ['l1', 'l2'], 'C': [0.001, 0.1, 1, 10]} on train/test set 0\n",
      "Running DT with params: {'criterion': ['gini', 'entropy'], 'max_depth': [1, 5, 10, 50, 100], 'min_samples_split': [2, 5]} on train/test set 0\n",
      "Running AB with params: {'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1, 10, 100]} on train/test set 0\n",
      "Running RF with params: {'n_estimators': [10, 100], 'max_depth': [5, 50], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 10], 'n_jobs': [-1]} on train/test set 0\n",
      "Running LR with params: {'penalty': ['l1', 'l2'], 'C': [0.001, 0.1, 1, 10]} on train/test set 1\n",
      "Running DT with params: {'criterion': ['gini', 'entropy'], 'max_depth': [1, 5, 10, 50, 100], 'min_samples_split': [2, 5]} on train/test set 1\n",
      "Running AB with params: {'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1, 10, 100]} on train/test set 1\n",
      "Running RF with params: {'n_estimators': [10, 100], 'max_depth': [5, 50], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 10], 'n_jobs': [-1]} on train/test set 1\n",
      "Running LR with params: {'penalty': ['l1', 'l2'], 'C': [0.001, 0.1, 1, 10]} on train/test set 2\n",
      "Running DT with params: {'criterion': ['gini', 'entropy'], 'max_depth': [1, 5, 10, 50, 100], 'min_samples_split': [2, 5]} on train/test set 2\n",
      "Running AB with params: {'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1, 10, 100]} on train/test set 2\n",
      "Running RF with params: {'n_estimators': [10, 100], 'max_depth': [5, 50], 'max_features': ['sqrt', 'log2'], 'min_samples_split': [2, 10], 'n_jobs': [-1]} on train/test set 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>train_test_split_threshold</th>\n",
       "      <th>p_at_1</th>\n",
       "      <th>r_at_1</th>\n",
       "      <th>f1_at_1</th>\n",
       "      <th>p_at_2</th>\n",
       "      <th>r_at_2</th>\n",
       "      <th>f1_at_2</th>\n",
       "      <th>...</th>\n",
       "      <th>p_at_20</th>\n",
       "      <th>r_at_20</th>\n",
       "      <th>f1_at_20</th>\n",
       "      <th>p_at_30</th>\n",
       "      <th>r_at_30</th>\n",
       "      <th>f1_at_30</th>\n",
       "      <th>p_at_50</th>\n",
       "      <th>r_at_50</th>\n",
       "      <th>f1_at_50</th>\n",
       "      <th>auc-roc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l1'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.861678</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>0.023723</td>\n",
       "      <td>0.840317</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.045692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768029</td>\n",
       "      <td>0.214718</td>\n",
       "      <td>0.335609</td>\n",
       "      <td>0.756981</td>\n",
       "      <td>0.317455</td>\n",
       "      <td>0.447319</td>\n",
       "      <td>0.730879</td>\n",
       "      <td>0.510840</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.537922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.001, 'penalty': 'l2'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.025721</td>\n",
       "      <td>0.925255</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.050311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858259</td>\n",
       "      <td>0.239943</td>\n",
       "      <td>0.375037</td>\n",
       "      <td>0.836906</td>\n",
       "      <td>0.350973</td>\n",
       "      <td>0.494548</td>\n",
       "      <td>0.799257</td>\n",
       "      <td>0.558633</td>\n",
       "      <td>0.657625</td>\n",
       "      <td>0.638292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.963719</td>\n",
       "      <td>0.013451</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.966025</td>\n",
       "      <td>0.026998</td>\n",
       "      <td>0.052528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902751</td>\n",
       "      <td>0.252382</td>\n",
       "      <td>0.394479</td>\n",
       "      <td>0.874792</td>\n",
       "      <td>0.366862</td>\n",
       "      <td>0.516936</td>\n",
       "      <td>0.819816</td>\n",
       "      <td>0.573002</td>\n",
       "      <td>0.674541</td>\n",
       "      <td>0.673043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l2'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.026408</td>\n",
       "      <td>0.967157</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.052589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899921</td>\n",
       "      <td>0.251590</td>\n",
       "      <td>0.393242</td>\n",
       "      <td>0.873208</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.820722</td>\n",
       "      <td>0.573635</td>\n",
       "      <td>0.675286</td>\n",
       "      <td>0.673600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.963719</td>\n",
       "      <td>0.013451</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.959230</td>\n",
       "      <td>0.026808</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903544</td>\n",
       "      <td>0.252603</td>\n",
       "      <td>0.394825</td>\n",
       "      <td>0.875472</td>\n",
       "      <td>0.367147</td>\n",
       "      <td>0.517337</td>\n",
       "      <td>0.819635</td>\n",
       "      <td>0.572875</td>\n",
       "      <td>0.674392</td>\n",
       "      <td>0.672994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.963719</td>\n",
       "      <td>0.013451</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.963760</td>\n",
       "      <td>0.026935</td>\n",
       "      <td>0.052405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903996</td>\n",
       "      <td>0.252730</td>\n",
       "      <td>0.395023</td>\n",
       "      <td>0.875170</td>\n",
       "      <td>0.367020</td>\n",
       "      <td>0.517159</td>\n",
       "      <td>0.819907</td>\n",
       "      <td>0.573065</td>\n",
       "      <td>0.674615</td>\n",
       "      <td>0.673964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.963719</td>\n",
       "      <td>0.013451</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.960362</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>0.052220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903430</td>\n",
       "      <td>0.252572</td>\n",
       "      <td>0.394776</td>\n",
       "      <td>0.875321</td>\n",
       "      <td>0.367083</td>\n",
       "      <td>0.517248</td>\n",
       "      <td>0.819997</td>\n",
       "      <td>0.573129</td>\n",
       "      <td>0.674690</td>\n",
       "      <td>0.673001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.026408</td>\n",
       "      <td>0.966025</td>\n",
       "      <td>0.026998</td>\n",
       "      <td>0.052528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899808</td>\n",
       "      <td>0.251559</td>\n",
       "      <td>0.393193</td>\n",
       "      <td>0.873962</td>\n",
       "      <td>0.366514</td>\n",
       "      <td>0.516446</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.573603</td>\n",
       "      <td>0.675249</td>\n",
       "      <td>0.673729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'min_sam...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.436974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419370</td>\n",
       "      <td>0.590924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.579323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 1, 'min_sam...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.436974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419370</td>\n",
       "      <td>0.590924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.579323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>0.967157</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.052589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904676</td>\n",
       "      <td>0.252920</td>\n",
       "      <td>0.395320</td>\n",
       "      <td>0.858038</td>\n",
       "      <td>0.359835</td>\n",
       "      <td>0.507035</td>\n",
       "      <td>0.811575</td>\n",
       "      <td>0.567242</td>\n",
       "      <td>0.667760</td>\n",
       "      <td>0.653808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'min_sam...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>0.967157</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.052589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904676</td>\n",
       "      <td>0.252920</td>\n",
       "      <td>0.395320</td>\n",
       "      <td>0.858038</td>\n",
       "      <td>0.359835</td>\n",
       "      <td>0.507035</td>\n",
       "      <td>0.811575</td>\n",
       "      <td>0.567242</td>\n",
       "      <td>0.667760</td>\n",
       "      <td>0.653808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.671202</td>\n",
       "      <td>0.009369</td>\n",
       "      <td>0.018479</td>\n",
       "      <td>0.767837</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.041751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851579</td>\n",
       "      <td>0.238076</td>\n",
       "      <td>0.372118</td>\n",
       "      <td>0.833811</td>\n",
       "      <td>0.349676</td>\n",
       "      <td>0.492719</td>\n",
       "      <td>0.805823</td>\n",
       "      <td>0.563222</td>\n",
       "      <td>0.663028</td>\n",
       "      <td>0.646482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_sa...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.768969</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.041813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851353</td>\n",
       "      <td>0.238012</td>\n",
       "      <td>0.372019</td>\n",
       "      <td>0.833887</td>\n",
       "      <td>0.349707</td>\n",
       "      <td>0.492764</td>\n",
       "      <td>0.806050</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.663214</td>\n",
       "      <td>0.646512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_sa...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.997732</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>0.027916</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948262</td>\n",
       "      <td>0.265105</td>\n",
       "      <td>0.414366</td>\n",
       "      <td>0.965434</td>\n",
       "      <td>0.404874</td>\n",
       "      <td>0.570498</td>\n",
       "      <td>0.979215</td>\n",
       "      <td>0.684412</td>\n",
       "      <td>0.805693</td>\n",
       "      <td>0.540092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 50, 'min_sa...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.997732</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>0.027916</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687422</td>\n",
       "      <td>0.192182</td>\n",
       "      <td>0.300386</td>\n",
       "      <td>0.752830</td>\n",
       "      <td>0.315715</td>\n",
       "      <td>0.444866</td>\n",
       "      <td>0.851651</td>\n",
       "      <td>0.595252</td>\n",
       "      <td>0.700734</td>\n",
       "      <td>0.547933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_s...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.997732</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>0.027916</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931960</td>\n",
       "      <td>0.260548</td>\n",
       "      <td>0.407243</td>\n",
       "      <td>0.954566</td>\n",
       "      <td>0.400317</td>\n",
       "      <td>0.564076</td>\n",
       "      <td>0.972694</td>\n",
       "      <td>0.679854</td>\n",
       "      <td>0.800328</td>\n",
       "      <td>0.536223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 100, 'min_s...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.997732</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>0.027916</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687422</td>\n",
       "      <td>0.192182</td>\n",
       "      <td>0.300386</td>\n",
       "      <td>0.747019</td>\n",
       "      <td>0.313277</td>\n",
       "      <td>0.441432</td>\n",
       "      <td>0.848164</td>\n",
       "      <td>0.592815</td>\n",
       "      <td>0.697865</td>\n",
       "      <td>0.547202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'min_...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.436974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419370</td>\n",
       "      <td>0.590924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.579323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 1, 'min_...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.436974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419370</td>\n",
       "      <td>0.590924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.579323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.977324</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>0.026907</td>\n",
       "      <td>0.984145</td>\n",
       "      <td>0.027504</td>\n",
       "      <td>0.053513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903544</td>\n",
       "      <td>0.252603</td>\n",
       "      <td>0.394825</td>\n",
       "      <td>0.855245</td>\n",
       "      <td>0.358664</td>\n",
       "      <td>0.505385</td>\n",
       "      <td>0.814427</td>\n",
       "      <td>0.569236</td>\n",
       "      <td>0.670107</td>\n",
       "      <td>0.653398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.977324</td>\n",
       "      <td>0.013641</td>\n",
       "      <td>0.026907</td>\n",
       "      <td>0.984145</td>\n",
       "      <td>0.027504</td>\n",
       "      <td>0.053513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903544</td>\n",
       "      <td>0.252603</td>\n",
       "      <td>0.394825</td>\n",
       "      <td>0.855245</td>\n",
       "      <td>0.358664</td>\n",
       "      <td>0.505385</td>\n",
       "      <td>0.814427</td>\n",
       "      <td>0.569236</td>\n",
       "      <td>0.670107</td>\n",
       "      <td>0.653398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.009084</td>\n",
       "      <td>0.017917</td>\n",
       "      <td>0.585504</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>0.031837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850221</td>\n",
       "      <td>0.237696</td>\n",
       "      <td>0.371525</td>\n",
       "      <td>0.846264</td>\n",
       "      <td>0.354898</td>\n",
       "      <td>0.500078</td>\n",
       "      <td>0.807907</td>\n",
       "      <td>0.564678</td>\n",
       "      <td>0.664742</td>\n",
       "      <td>0.647379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.648526</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>0.583239</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.031714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849994</td>\n",
       "      <td>0.237633</td>\n",
       "      <td>0.371426</td>\n",
       "      <td>0.846189</td>\n",
       "      <td>0.354866</td>\n",
       "      <td>0.500033</td>\n",
       "      <td>0.807816</td>\n",
       "      <td>0.564615</td>\n",
       "      <td>0.664667</td>\n",
       "      <td>0.647223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.436974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419370</td>\n",
       "      <td>0.590924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.540259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.997732</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>0.027916</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795087</td>\n",
       "      <td>0.222282</td>\n",
       "      <td>0.347432</td>\n",
       "      <td>0.863321</td>\n",
       "      <td>0.362051</td>\n",
       "      <td>0.510157</td>\n",
       "      <td>0.917946</td>\n",
       "      <td>0.641589</td>\n",
       "      <td>0.755281</td>\n",
       "      <td>0.548255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'mi...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.436974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419370</td>\n",
       "      <td>0.590924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.543410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'mi...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.997732</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>0.027916</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819088</td>\n",
       "      <td>0.228992</td>\n",
       "      <td>0.357920</td>\n",
       "      <td>0.879321</td>\n",
       "      <td>0.368761</td>\n",
       "      <td>0.519612</td>\n",
       "      <td>0.927546</td>\n",
       "      <td>0.648299</td>\n",
       "      <td>0.763180</td>\n",
       "      <td>0.546703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 1}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.436974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.419370</td>\n",
       "      <td>0.590924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.822795</td>\n",
       "      <td>0.579323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 10}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013958</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.858259</td>\n",
       "      <td>0.239943</td>\n",
       "      <td>0.375037</td>\n",
       "      <td>0.823698</td>\n",
       "      <td>0.345434</td>\n",
       "      <td>0.486743</td>\n",
       "      <td>0.818684</td>\n",
       "      <td>0.572211</td>\n",
       "      <td>0.673609</td>\n",
       "      <td>0.651652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.788110</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.041266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749201</td>\n",
       "      <td>0.201631</td>\n",
       "      <td>0.317747</td>\n",
       "      <td>0.829865</td>\n",
       "      <td>0.335027</td>\n",
       "      <td>0.477344</td>\n",
       "      <td>0.897923</td>\n",
       "      <td>0.604197</td>\n",
       "      <td>0.722342</td>\n",
       "      <td>0.605496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 5, 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.788110</td>\n",
       "      <td>0.021188</td>\n",
       "      <td>0.041266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749201</td>\n",
       "      <td>0.201631</td>\n",
       "      <td>0.317747</td>\n",
       "      <td>0.829865</td>\n",
       "      <td>0.335027</td>\n",
       "      <td>0.477344</td>\n",
       "      <td>0.897923</td>\n",
       "      <td>0.604197</td>\n",
       "      <td>0.722342</td>\n",
       "      <td>0.605496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835998</td>\n",
       "      <td>0.224991</td>\n",
       "      <td>0.354560</td>\n",
       "      <td>0.821744</td>\n",
       "      <td>0.331749</td>\n",
       "      <td>0.472673</td>\n",
       "      <td>0.815458</td>\n",
       "      <td>0.548707</td>\n",
       "      <td>0.656002</td>\n",
       "      <td>0.613331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 10, 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838282</td>\n",
       "      <td>0.225606</td>\n",
       "      <td>0.355528</td>\n",
       "      <td>0.823267</td>\n",
       "      <td>0.332363</td>\n",
       "      <td>0.473549</td>\n",
       "      <td>0.815031</td>\n",
       "      <td>0.548420</td>\n",
       "      <td>0.655659</td>\n",
       "      <td>0.612698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269128</td>\n",
       "      <td>0.424115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403713</td>\n",
       "      <td>0.575207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672882</td>\n",
       "      <td>0.804459</td>\n",
       "      <td>0.545247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 50, 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.996951</td>\n",
       "      <td>0.013401</td>\n",
       "      <td>0.026447</td>\n",
       "      <td>0.998476</td>\n",
       "      <td>0.026843</td>\n",
       "      <td>0.052281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976549</td>\n",
       "      <td>0.262817</td>\n",
       "      <td>0.414169</td>\n",
       "      <td>0.984367</td>\n",
       "      <td>0.397402</td>\n",
       "      <td>0.566215</td>\n",
       "      <td>0.990560</td>\n",
       "      <td>0.666530</td>\n",
       "      <td>0.796864</td>\n",
       "      <td>0.549429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'mi...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269128</td>\n",
       "      <td>0.424115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403713</td>\n",
       "      <td>0.575207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672882</td>\n",
       "      <td>0.804459</td>\n",
       "      <td>0.545247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>DT</td>\n",
       "      <td>DecisionTreeClassifier(class_weight=None, crit...</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 100, 'mi...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.996951</td>\n",
       "      <td>0.013401</td>\n",
       "      <td>0.026447</td>\n",
       "      <td>0.998476</td>\n",
       "      <td>0.026843</td>\n",
       "      <td>0.052281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976549</td>\n",
       "      <td>0.262817</td>\n",
       "      <td>0.414169</td>\n",
       "      <td>0.984367</td>\n",
       "      <td>0.397402</td>\n",
       "      <td>0.566215</td>\n",
       "      <td>0.990560</td>\n",
       "      <td>0.666530</td>\n",
       "      <td>0.796864</td>\n",
       "      <td>0.549429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 1}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269128</td>\n",
       "      <td>0.424115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403713</td>\n",
       "      <td>0.575207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672882</td>\n",
       "      <td>0.804459</td>\n",
       "      <td>0.559447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 10}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845135</td>\n",
       "      <td>0.227450</td>\n",
       "      <td>0.358435</td>\n",
       "      <td>0.853822</td>\n",
       "      <td>0.344699</td>\n",
       "      <td>0.491125</td>\n",
       "      <td>0.837871</td>\n",
       "      <td>0.563788</td>\n",
       "      <td>0.674032</td>\n",
       "      <td>0.624234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME', 'n_estimators': 100}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.954268</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>0.025314</td>\n",
       "      <td>0.957317</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.050126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854576</td>\n",
       "      <td>0.229991</td>\n",
       "      <td>0.362439</td>\n",
       "      <td>0.855345</td>\n",
       "      <td>0.345314</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.821609</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.660951</td>\n",
       "      <td>0.637534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 1}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269128</td>\n",
       "      <td>0.424115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403713</td>\n",
       "      <td>0.575207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672882</td>\n",
       "      <td>0.804459</td>\n",
       "      <td>0.559447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 10}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026884</td>\n",
       "      <td>0.052361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865692</td>\n",
       "      <td>0.232982</td>\n",
       "      <td>0.367153</td>\n",
       "      <td>0.833113</td>\n",
       "      <td>0.336339</td>\n",
       "      <td>0.479213</td>\n",
       "      <td>0.835435</td>\n",
       "      <td>0.562149</td>\n",
       "      <td>0.672073</td>\n",
       "      <td>0.635815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 100}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.972561</td>\n",
       "      <td>0.013073</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.961890</td>\n",
       "      <td>0.025860</td>\n",
       "      <td>0.050365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897518</td>\n",
       "      <td>0.241547</td>\n",
       "      <td>0.380651</td>\n",
       "      <td>0.872906</td>\n",
       "      <td>0.352404</td>\n",
       "      <td>0.502102</td>\n",
       "      <td>0.830319</td>\n",
       "      <td>0.558707</td>\n",
       "      <td>0.667957</td>\n",
       "      <td>0.653333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>0.023212</td>\n",
       "      <td>0.876524</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.045895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845135</td>\n",
       "      <td>0.227450</td>\n",
       "      <td>0.358435</td>\n",
       "      <td>0.834839</td>\n",
       "      <td>0.337035</td>\n",
       "      <td>0.480206</td>\n",
       "      <td>0.815823</td>\n",
       "      <td>0.548953</td>\n",
       "      <td>0.656296</td>\n",
       "      <td>0.626015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>0.025234</td>\n",
       "      <td>0.917683</td>\n",
       "      <td>0.024671</td>\n",
       "      <td>0.048050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864626</td>\n",
       "      <td>0.232695</td>\n",
       "      <td>0.366701</td>\n",
       "      <td>0.852096</td>\n",
       "      <td>0.344002</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.823984</td>\n",
       "      <td>0.554444</td>\n",
       "      <td>0.662861</td>\n",
       "      <td>0.642483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.905488</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.024020</td>\n",
       "      <td>0.873476</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.045736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845592</td>\n",
       "      <td>0.227573</td>\n",
       "      <td>0.358628</td>\n",
       "      <td>0.835753</td>\n",
       "      <td>0.337404</td>\n",
       "      <td>0.480731</td>\n",
       "      <td>0.817468</td>\n",
       "      <td>0.550059</td>\n",
       "      <td>0.657619</td>\n",
       "      <td>0.629299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'sqrt', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.948171</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.025153</td>\n",
       "      <td>0.916159</td>\n",
       "      <td>0.024630</td>\n",
       "      <td>0.047971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865235</td>\n",
       "      <td>0.232859</td>\n",
       "      <td>0.366959</td>\n",
       "      <td>0.852299</td>\n",
       "      <td>0.344084</td>\n",
       "      <td>0.490249</td>\n",
       "      <td>0.824106</td>\n",
       "      <td>0.554526</td>\n",
       "      <td>0.662959</td>\n",
       "      <td>0.642183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.887195</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.023535</td>\n",
       "      <td>0.876524</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.045895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856251</td>\n",
       "      <td>0.230441</td>\n",
       "      <td>0.363149</td>\n",
       "      <td>0.837275</td>\n",
       "      <td>0.338019</td>\n",
       "      <td>0.481607</td>\n",
       "      <td>0.818564</td>\n",
       "      <td>0.550797</td>\n",
       "      <td>0.658501</td>\n",
       "      <td>0.634428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.896341</td>\n",
       "      <td>0.012049</td>\n",
       "      <td>0.023778</td>\n",
       "      <td>0.882622</td>\n",
       "      <td>0.023729</td>\n",
       "      <td>0.046215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862190</td>\n",
       "      <td>0.232040</td>\n",
       "      <td>0.365668</td>\n",
       "      <td>0.846818</td>\n",
       "      <td>0.341871</td>\n",
       "      <td>0.487096</td>\n",
       "      <td>0.822157</td>\n",
       "      <td>0.553215</td>\n",
       "      <td>0.661391</td>\n",
       "      <td>0.640744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.899390</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.023859</td>\n",
       "      <td>0.903963</td>\n",
       "      <td>0.024302</td>\n",
       "      <td>0.047332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851683</td>\n",
       "      <td>0.229212</td>\n",
       "      <td>0.361212</td>\n",
       "      <td>0.836565</td>\n",
       "      <td>0.337732</td>\n",
       "      <td>0.481198</td>\n",
       "      <td>0.818259</td>\n",
       "      <td>0.550592</td>\n",
       "      <td>0.658256</td>\n",
       "      <td>0.634022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 'log2', 'min_...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.905488</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.024020</td>\n",
       "      <td>0.908537</td>\n",
       "      <td>0.024425</td>\n",
       "      <td>0.047572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865388</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.367024</td>\n",
       "      <td>0.846513</td>\n",
       "      <td>0.341748</td>\n",
       "      <td>0.486920</td>\n",
       "      <td>0.821244</td>\n",
       "      <td>0.552600</td>\n",
       "      <td>0.660657</td>\n",
       "      <td>0.640944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.669207</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>0.035040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841785</td>\n",
       "      <td>0.226548</td>\n",
       "      <td>0.357014</td>\n",
       "      <td>0.758096</td>\n",
       "      <td>0.306053</td>\n",
       "      <td>0.436062</td>\n",
       "      <td>0.853889</td>\n",
       "      <td>0.574567</td>\n",
       "      <td>0.686918</td>\n",
       "      <td>0.580721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>0.024587</td>\n",
       "      <td>0.917683</td>\n",
       "      <td>0.024671</td>\n",
       "      <td>0.048050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853205</td>\n",
       "      <td>0.229622</td>\n",
       "      <td>0.361857</td>\n",
       "      <td>0.830677</td>\n",
       "      <td>0.335355</td>\n",
       "      <td>0.477812</td>\n",
       "      <td>0.807418</td>\n",
       "      <td>0.543297</td>\n",
       "      <td>0.649535</td>\n",
       "      <td>0.615840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.884146</td>\n",
       "      <td>0.011885</td>\n",
       "      <td>0.023454</td>\n",
       "      <td>0.876524</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.045895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831430</td>\n",
       "      <td>0.223761</td>\n",
       "      <td>0.352622</td>\n",
       "      <td>0.826109</td>\n",
       "      <td>0.333511</td>\n",
       "      <td>0.475184</td>\n",
       "      <td>0.799257</td>\n",
       "      <td>0.537806</td>\n",
       "      <td>0.642969</td>\n",
       "      <td>0.603359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'sqrt', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.932927</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>0.024748</td>\n",
       "      <td>0.925305</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.048450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872697</td>\n",
       "      <td>0.234867</td>\n",
       "      <td>0.370124</td>\n",
       "      <td>0.849254</td>\n",
       "      <td>0.342855</td>\n",
       "      <td>0.488497</td>\n",
       "      <td>0.815336</td>\n",
       "      <td>0.548625</td>\n",
       "      <td>0.655904</td>\n",
       "      <td>0.632106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.704268</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.227204</td>\n",
       "      <td>0.358047</td>\n",
       "      <td>0.767739</td>\n",
       "      <td>0.309946</td>\n",
       "      <td>0.441609</td>\n",
       "      <td>0.843779</td>\n",
       "      <td>0.567764</td>\n",
       "      <td>0.678785</td>\n",
       "      <td>0.571762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>0.023212</td>\n",
       "      <td>0.858232</td>\n",
       "      <td>0.023073</td>\n",
       "      <td>0.044938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838587</td>\n",
       "      <td>0.225687</td>\n",
       "      <td>0.355657</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.331462</td>\n",
       "      <td>0.472264</td>\n",
       "      <td>0.810463</td>\n",
       "      <td>0.545347</td>\n",
       "      <td>0.651984</td>\n",
       "      <td>0.606153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.011967</td>\n",
       "      <td>0.023616</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.023933</td>\n",
       "      <td>0.046614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842089</td>\n",
       "      <td>0.226630</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.827023</td>\n",
       "      <td>0.333880</td>\n",
       "      <td>0.475709</td>\n",
       "      <td>0.804373</td>\n",
       "      <td>0.541248</td>\n",
       "      <td>0.647085</td>\n",
       "      <td>0.607609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>RF</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'max_depth': 50, 'max_features': 'log2', 'min...</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.012131</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>0.916159</td>\n",
       "      <td>0.024630</td>\n",
       "      <td>0.047971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865083</td>\n",
       "      <td>0.232818</td>\n",
       "      <td>0.366895</td>\n",
       "      <td>0.847224</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.487329</td>\n",
       "      <td>0.814361</td>\n",
       "      <td>0.547969</td>\n",
       "      <td>0.655120</td>\n",
       "      <td>0.632334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name                                              model  \\\n",
       "0           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "1           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "2           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "3           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "4           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "5           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "6           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "7           LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "8           DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "9           DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "10          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "11          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "12          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "13          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "14          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "15          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "16          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "17          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "18          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "19          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "20          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "21          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "22          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "23          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "24          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "25          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "26          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "27          DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "28          AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "29          AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "..         ...                                                ...   \n",
       "120         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "121         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "122         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "123         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "124         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "125         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "126         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "127         DT  DecisionTreeClassifier(class_weight=None, crit...   \n",
       "128         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "129         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "130         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "131         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "132         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "133         AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "134         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "135         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "136         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "137         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "138         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "139         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "140         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "141         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "142         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "143         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "144         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "145         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "146         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "147         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "148         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "149         RF  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "\n",
       "                                            parameters  \\\n",
       "0                        {'C': 0.001, 'penalty': 'l1'}   \n",
       "1                        {'C': 0.001, 'penalty': 'l2'}   \n",
       "2                          {'C': 0.1, 'penalty': 'l1'}   \n",
       "3                          {'C': 0.1, 'penalty': 'l2'}   \n",
       "4                            {'C': 1, 'penalty': 'l1'}   \n",
       "5                            {'C': 1, 'penalty': 'l2'}   \n",
       "6                           {'C': 10, 'penalty': 'l1'}   \n",
       "7                           {'C': 10, 'penalty': 'l2'}   \n",
       "8    {'criterion': 'gini', 'max_depth': 1, 'min_sam...   \n",
       "9    {'criterion': 'gini', 'max_depth': 1, 'min_sam...   \n",
       "10   {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "11   {'criterion': 'gini', 'max_depth': 5, 'min_sam...   \n",
       "12   {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "13   {'criterion': 'gini', 'max_depth': 10, 'min_sa...   \n",
       "14   {'criterion': 'gini', 'max_depth': 50, 'min_sa...   \n",
       "15   {'criterion': 'gini', 'max_depth': 50, 'min_sa...   \n",
       "16   {'criterion': 'gini', 'max_depth': 100, 'min_s...   \n",
       "17   {'criterion': 'gini', 'max_depth': 100, 'min_s...   \n",
       "18   {'criterion': 'entropy', 'max_depth': 1, 'min_...   \n",
       "19   {'criterion': 'entropy', 'max_depth': 1, 'min_...   \n",
       "20   {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "21   {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "22   {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "23   {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "24   {'criterion': 'entropy', 'max_depth': 50, 'min...   \n",
       "25   {'criterion': 'entropy', 'max_depth': 50, 'min...   \n",
       "26   {'criterion': 'entropy', 'max_depth': 100, 'mi...   \n",
       "27   {'criterion': 'entropy', 'max_depth': 100, 'mi...   \n",
       "28           {'algorithm': 'SAMME', 'n_estimators': 1}   \n",
       "29          {'algorithm': 'SAMME', 'n_estimators': 10}   \n",
       "..                                                 ...   \n",
       "120  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "121  {'criterion': 'entropy', 'max_depth': 5, 'min_...   \n",
       "122  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "123  {'criterion': 'entropy', 'max_depth': 10, 'min...   \n",
       "124  {'criterion': 'entropy', 'max_depth': 50, 'min...   \n",
       "125  {'criterion': 'entropy', 'max_depth': 50, 'min...   \n",
       "126  {'criterion': 'entropy', 'max_depth': 100, 'mi...   \n",
       "127  {'criterion': 'entropy', 'max_depth': 100, 'mi...   \n",
       "128          {'algorithm': 'SAMME', 'n_estimators': 1}   \n",
       "129         {'algorithm': 'SAMME', 'n_estimators': 10}   \n",
       "130        {'algorithm': 'SAMME', 'n_estimators': 100}   \n",
       "131        {'algorithm': 'SAMME.R', 'n_estimators': 1}   \n",
       "132       {'algorithm': 'SAMME.R', 'n_estimators': 10}   \n",
       "133      {'algorithm': 'SAMME.R', 'n_estimators': 100}   \n",
       "134  {'max_depth': 5, 'max_features': 'sqrt', 'min_...   \n",
       "135  {'max_depth': 5, 'max_features': 'sqrt', 'min_...   \n",
       "136  {'max_depth': 5, 'max_features': 'sqrt', 'min_...   \n",
       "137  {'max_depth': 5, 'max_features': 'sqrt', 'min_...   \n",
       "138  {'max_depth': 5, 'max_features': 'log2', 'min_...   \n",
       "139  {'max_depth': 5, 'max_features': 'log2', 'min_...   \n",
       "140  {'max_depth': 5, 'max_features': 'log2', 'min_...   \n",
       "141  {'max_depth': 5, 'max_features': 'log2', 'min_...   \n",
       "142  {'max_depth': 50, 'max_features': 'sqrt', 'min...   \n",
       "143  {'max_depth': 50, 'max_features': 'sqrt', 'min...   \n",
       "144  {'max_depth': 50, 'max_features': 'sqrt', 'min...   \n",
       "145  {'max_depth': 50, 'max_features': 'sqrt', 'min...   \n",
       "146  {'max_depth': 50, 'max_features': 'log2', 'min...   \n",
       "147  {'max_depth': 50, 'max_features': 'log2', 'min...   \n",
       "148  {'max_depth': 50, 'max_features': 'log2', 'min...   \n",
       "149  {'max_depth': 50, 'max_features': 'log2', 'min...   \n",
       "\n",
       "    train_test_split_threshold    p_at_1    r_at_1   f1_at_1    p_at_2  \\\n",
       "0                   2013-07-01  0.861678  0.012027  0.023723  0.840317   \n",
       "1                   2013-07-01  0.934240  0.013040  0.025721  0.925255   \n",
       "2                   2013-07-01  0.963719  0.013451  0.026533  0.966025   \n",
       "3                   2013-07-01  0.959184  0.013388  0.026408  0.967157   \n",
       "4                   2013-07-01  0.963719  0.013451  0.026533  0.959230   \n",
       "5                   2013-07-01  0.963719  0.013451  0.026533  0.963760   \n",
       "6                   2013-07-01  0.963719  0.013451  0.026533  0.960362   \n",
       "7                   2013-07-01  0.959184  0.013388  0.026408  0.966025   \n",
       "8                   2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "9                   2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "10                  2013-07-01  1.000000  0.013958  0.027532  0.967157   \n",
       "11                  2013-07-01  1.000000  0.013958  0.027532  0.967157   \n",
       "12                  2013-07-01  0.671202  0.009369  0.018479  0.767837   \n",
       "13                  2013-07-01  0.673469  0.009400  0.018542  0.768969   \n",
       "14                  2013-07-01  0.997732  0.013926  0.027469  0.998867   \n",
       "15                  2013-07-01  0.997732  0.013926  0.027469  0.998867   \n",
       "16                  2013-07-01  0.997732  0.013926  0.027469  0.998867   \n",
       "17                  2013-07-01  0.997732  0.013926  0.027469  0.998867   \n",
       "18                  2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "19                  2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "20                  2013-07-01  0.977324  0.013641  0.026907  0.984145   \n",
       "21                  2013-07-01  0.977324  0.013641  0.026907  0.984145   \n",
       "22                  2013-07-01  0.650794  0.009084  0.017917  0.585504   \n",
       "23                  2013-07-01  0.648526  0.009052  0.017855  0.583239   \n",
       "24                  2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "25                  2013-07-01  0.997732  0.013926  0.027469  0.998867   \n",
       "26                  2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "27                  2013-07-01  0.997732  0.013926  0.027469  0.998867   \n",
       "28                  2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "29                  2013-07-01  1.000000  0.013958  0.027532  1.000000   \n",
       "..                         ...       ...       ...       ...       ...   \n",
       "120                 2012-07-01  0.963415  0.012950  0.025557  0.788110   \n",
       "121                 2012-07-01  0.963415  0.012950  0.025557  0.788110   \n",
       "122                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "123                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "124                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "125                 2012-07-01  0.996951  0.013401  0.026447  0.998476   \n",
       "126                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "127                 2012-07-01  0.996951  0.013401  0.026447  0.998476   \n",
       "128                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "129                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "130                 2012-07-01  0.954268  0.012827  0.025314  0.957317   \n",
       "131                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "132                 2012-07-01  1.000000  0.013442  0.026528  1.000000   \n",
       "133                 2012-07-01  0.972561  0.013073  0.025800  0.961890   \n",
       "134                 2012-07-01  0.875000  0.011762  0.023212  0.876524   \n",
       "135                 2012-07-01  0.951220  0.012786  0.025234  0.917683   \n",
       "136                 2012-07-01  0.905488  0.012172  0.024020  0.873476   \n",
       "137                 2012-07-01  0.948171  0.012745  0.025153  0.916159   \n",
       "138                 2012-07-01  0.887195  0.011926  0.023535  0.876524   \n",
       "139                 2012-07-01  0.896341  0.012049  0.023778  0.882622   \n",
       "140                 2012-07-01  0.899390  0.012090  0.023859  0.903963   \n",
       "141                 2012-07-01  0.905488  0.012172  0.024020  0.908537   \n",
       "142                 2012-07-01  1.000000  0.013442  0.026528  0.669207   \n",
       "143                 2012-07-01  0.926829  0.012459  0.024587  0.917683   \n",
       "144                 2012-07-01  0.884146  0.011885  0.023454  0.876524   \n",
       "145                 2012-07-01  0.932927  0.012540  0.024748  0.925305   \n",
       "146                 2012-07-01  1.000000  0.013442  0.026528  0.704268   \n",
       "147                 2012-07-01  0.875000  0.011762  0.023212  0.858232   \n",
       "148                 2012-07-01  0.890244  0.011967  0.023616  0.890244   \n",
       "149                 2012-07-01  0.902439  0.012131  0.023940  0.916159   \n",
       "\n",
       "       r_at_2   f1_at_2    ...      p_at_20   r_at_20  f1_at_20   p_at_30  \\\n",
       "0    0.023485  0.045692    ...     0.768029  0.214718  0.335609  0.756981   \n",
       "1    0.025859  0.050311    ...     0.858259  0.239943  0.375037  0.836906   \n",
       "2    0.026998  0.052528    ...     0.902751  0.252382  0.394479  0.874792   \n",
       "3    0.027030  0.052589    ...     0.899921  0.251590  0.393242  0.873208   \n",
       "4    0.026808  0.052158    ...     0.903544  0.252603  0.394825  0.875472   \n",
       "5    0.026935  0.052405    ...     0.903996  0.252730  0.395023  0.875170   \n",
       "6    0.026840  0.052220    ...     0.903430  0.252572  0.394776  0.875321   \n",
       "7    0.026998  0.052528    ...     0.899808  0.251559  0.393193  0.873962   \n",
       "8    0.027947  0.054375    ...     1.000000  0.279570  0.436974  1.000000   \n",
       "9    0.027947  0.054375    ...     1.000000  0.279570  0.436974  1.000000   \n",
       "10   0.027030  0.052589    ...     0.904676  0.252920  0.395320  0.858038   \n",
       "11   0.027030  0.052589    ...     0.904676  0.252920  0.395320  0.858038   \n",
       "12   0.021459  0.041751    ...     0.851579  0.238076  0.372118  0.833811   \n",
       "13   0.021491  0.041813    ...     0.851353  0.238012  0.372019  0.833887   \n",
       "14   0.027916  0.054314    ...     0.948262  0.265105  0.414366  0.965434   \n",
       "15   0.027916  0.054314    ...     0.687422  0.192182  0.300386  0.752830   \n",
       "16   0.027916  0.054314    ...     0.931960  0.260548  0.407243  0.954566   \n",
       "17   0.027916  0.054314    ...     0.687422  0.192182  0.300386  0.747019   \n",
       "18   0.027947  0.054375    ...     1.000000  0.279570  0.436974  1.000000   \n",
       "19   0.027947  0.054375    ...     1.000000  0.279570  0.436974  1.000000   \n",
       "20   0.027504  0.053513    ...     0.903544  0.252603  0.394825  0.855245   \n",
       "21   0.027504  0.053513    ...     0.903544  0.252603  0.394825  0.855245   \n",
       "22   0.016363  0.031837    ...     0.850221  0.237696  0.371525  0.846264   \n",
       "23   0.016300  0.031714    ...     0.849994  0.237633  0.371426  0.846189   \n",
       "24   0.027947  0.054375    ...     1.000000  0.279570  0.436974  1.000000   \n",
       "25   0.027916  0.054314    ...     0.795087  0.222282  0.347432  0.863321   \n",
       "26   0.027947  0.054375    ...     1.000000  0.279570  0.436974  1.000000   \n",
       "27   0.027916  0.054314    ...     0.819088  0.228992  0.357920  0.879321   \n",
       "28   0.027947  0.054375    ...     1.000000  0.279570  0.436974  1.000000   \n",
       "29   0.027947  0.054375    ...     0.858259  0.239943  0.375037  0.823698   \n",
       "..        ...       ...    ...          ...       ...       ...       ...   \n",
       "120  0.021188  0.041266    ...     0.749201  0.201631  0.317747  0.829865   \n",
       "121  0.021188  0.041266    ...     0.749201  0.201631  0.317747  0.829865   \n",
       "122  0.026884  0.052361    ...     0.835998  0.224991  0.354560  0.821744   \n",
       "123  0.026884  0.052361    ...     0.838282  0.225606  0.355528  0.823267   \n",
       "124  0.026884  0.052361    ...     1.000000  0.269128  0.424115  1.000000   \n",
       "125  0.026843  0.052281    ...     0.976549  0.262817  0.414169  0.984367   \n",
       "126  0.026884  0.052361    ...     1.000000  0.269128  0.424115  1.000000   \n",
       "127  0.026843  0.052281    ...     0.976549  0.262817  0.414169  0.984367   \n",
       "128  0.026884  0.052361    ...     1.000000  0.269128  0.424115  1.000000   \n",
       "129  0.026884  0.052361    ...     0.845135  0.227450  0.358435  0.853822   \n",
       "130  0.025737  0.050126    ...     0.854576  0.229991  0.362439  0.855345   \n",
       "131  0.026884  0.052361    ...     1.000000  0.269128  0.424115  1.000000   \n",
       "132  0.026884  0.052361    ...     0.865692  0.232982  0.367153  0.833113   \n",
       "133  0.025860  0.050365    ...     0.897518  0.241547  0.380651  0.872906   \n",
       "134  0.023565  0.045895    ...     0.845135  0.227450  0.358435  0.834839   \n",
       "135  0.024671  0.048050    ...     0.864626  0.232695  0.366701  0.852096   \n",
       "136  0.023483  0.045736    ...     0.845592  0.227573  0.358628  0.835753   \n",
       "137  0.024630  0.047971    ...     0.865235  0.232859  0.366959  0.852299   \n",
       "138  0.023565  0.045895    ...     0.856251  0.230441  0.363149  0.837275   \n",
       "139  0.023729  0.046215    ...     0.862190  0.232040  0.365668  0.846818   \n",
       "140  0.024302  0.047332    ...     0.851683  0.229212  0.361212  0.836565   \n",
       "141  0.024425  0.047572    ...     0.865388  0.232900  0.367024  0.846513   \n",
       "142  0.017991  0.035040    ...     0.841785  0.226548  0.357014  0.758096   \n",
       "143  0.024671  0.048050    ...     0.853205  0.229622  0.361857  0.830677   \n",
       "144  0.023565  0.045895    ...     0.831430  0.223761  0.352622  0.826109   \n",
       "145  0.024876  0.048450    ...     0.872697  0.234867  0.370124  0.849254   \n",
       "146  0.018934  0.036876    ...     0.844221  0.227204  0.358047  0.767739   \n",
       "147  0.023073  0.044938    ...     0.838587  0.225687  0.355657  0.821033   \n",
       "148  0.023933  0.046614    ...     0.842089  0.226630  0.357143  0.827023   \n",
       "149  0.024630  0.047971    ...     0.865083  0.232818  0.366895  0.847224   \n",
       "\n",
       "      r_at_30  f1_at_30   p_at_50   r_at_50  f1_at_50   auc-roc  \n",
       "0    0.317455  0.447319  0.730879  0.510840  0.601364  0.537922  \n",
       "1    0.350973  0.494548  0.799257  0.558633  0.657625  0.638292  \n",
       "2    0.366862  0.516936  0.819816  0.573002  0.674541  0.673043  \n",
       "3    0.366197  0.516000  0.820722  0.573635  0.675286  0.673600  \n",
       "4    0.367147  0.517337  0.819635  0.572875  0.674392  0.672994  \n",
       "5    0.367020  0.517159  0.819907  0.573065  0.674615  0.673964  \n",
       "6    0.367083  0.517248  0.819997  0.573129  0.674690  0.673001  \n",
       "7    0.366514  0.516446  0.820677  0.573603  0.675249  0.673729  \n",
       "8    0.419370  0.590924  1.000000  0.698940  0.822795  0.579323  \n",
       "9    0.419370  0.590924  1.000000  0.698940  0.822795  0.579323  \n",
       "10   0.359835  0.507035  0.811575  0.567242  0.667760  0.653808  \n",
       "11   0.359835  0.507035  0.811575  0.567242  0.667760  0.653808  \n",
       "12   0.349676  0.492719  0.805823  0.563222  0.663028  0.646482  \n",
       "13   0.349707  0.492764  0.806050  0.563380  0.663214  0.646512  \n",
       "14   0.404874  0.570498  0.979215  0.684412  0.805693  0.540092  \n",
       "15   0.315715  0.444866  0.851651  0.595252  0.700734  0.547933  \n",
       "16   0.400317  0.564076  0.972694  0.679854  0.800328  0.536223  \n",
       "17   0.313277  0.441432  0.848164  0.592815  0.697865  0.547202  \n",
       "18   0.419370  0.590924  1.000000  0.698940  0.822795  0.579323  \n",
       "19   0.419370  0.590924  1.000000  0.698940  0.822795  0.579323  \n",
       "20   0.358664  0.505385  0.814427  0.569236  0.670107  0.653398  \n",
       "21   0.358664  0.505385  0.814427  0.569236  0.670107  0.653398  \n",
       "22   0.354898  0.500078  0.807907  0.564678  0.664742  0.647379  \n",
       "23   0.354866  0.500033  0.807816  0.564615  0.664667  0.647223  \n",
       "24   0.419370  0.590924  1.000000  0.698940  0.822795  0.540259  \n",
       "25   0.362051  0.510157  0.917946  0.641589  0.755281  0.548255  \n",
       "26   0.419370  0.590924  1.000000  0.698940  0.822795  0.543410  \n",
       "27   0.368761  0.519612  0.927546  0.648299  0.763180  0.546703  \n",
       "28   0.419370  0.590924  1.000000  0.698940  0.822795  0.579323  \n",
       "29   0.345434  0.486743  0.818684  0.572211  0.673609  0.651652  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "120  0.335027  0.477344  0.897923  0.604197  0.722342  0.605496  \n",
       "121  0.335027  0.477344  0.897923  0.604197  0.722342  0.605496  \n",
       "122  0.331749  0.472673  0.815458  0.548707  0.656002  0.613331  \n",
       "123  0.332363  0.473549  0.815031  0.548420  0.655659  0.612698  \n",
       "124  0.403713  0.575207  1.000000  0.672882  0.804459  0.545247  \n",
       "125  0.397402  0.566215  0.990560  0.666530  0.796864  0.549429  \n",
       "126  0.403713  0.575207  1.000000  0.672882  0.804459  0.545247  \n",
       "127  0.397402  0.566215  0.990560  0.666530  0.796864  0.549429  \n",
       "128  0.403713  0.575207  1.000000  0.672882  0.804459  0.559447  \n",
       "129  0.344699  0.491125  0.837871  0.563788  0.674032  0.624234  \n",
       "130  0.345314  0.492000  0.821609  0.552846  0.660951  0.637534  \n",
       "131  0.403713  0.575207  1.000000  0.672882  0.804459  0.559447  \n",
       "132  0.336339  0.479213  0.835435  0.562149  0.672073  0.635815  \n",
       "133  0.352404  0.502102  0.830319  0.558707  0.667957  0.653333  \n",
       "134  0.337035  0.480206  0.815823  0.548953  0.656296  0.626015  \n",
       "135  0.344002  0.490132  0.823984  0.554444  0.662861  0.642483  \n",
       "136  0.337404  0.480731  0.817468  0.550059  0.657619  0.629299  \n",
       "137  0.344084  0.490249  0.824106  0.554526  0.662959  0.642183  \n",
       "138  0.338019  0.481607  0.818564  0.550797  0.658501  0.634428  \n",
       "139  0.341871  0.487096  0.822157  0.553215  0.661391  0.640744  \n",
       "140  0.337732  0.481198  0.818259  0.550592  0.658256  0.634022  \n",
       "141  0.341748  0.486920  0.821244  0.552600  0.660657  0.640944  \n",
       "142  0.306053  0.436062  0.853889  0.574567  0.686918  0.580721  \n",
       "143  0.335355  0.477812  0.807418  0.543297  0.649535  0.615840  \n",
       "144  0.333511  0.475184  0.799257  0.537806  0.642969  0.603359  \n",
       "145  0.342855  0.488497  0.815336  0.548625  0.655904  0.632106  \n",
       "146  0.309946  0.441609  0.843779  0.567764  0.678785  0.571762  \n",
       "147  0.331462  0.472264  0.810463  0.545347  0.651984  0.606153  \n",
       "148  0.333880  0.475709  0.804373  0.541248  0.647085  0.607609  \n",
       "149  0.342035  0.487329  0.814361  0.547969  0.655120  0.632334  \n",
       "\n",
       "[150 rows x 26 columns]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(pipeline)\n",
    "\n",
    "results = pipeline.iterate_over_models_and_training_test_sets(models_to_run, models, parameters_grid, train_test_sets)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe best models for each train/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>train_test_split_threshold</th>\n",
       "      <th>p_at_1</th>\n",
       "      <th>r_at_1</th>\n",
       "      <th>f1_at_1</th>\n",
       "      <th>p_at_2</th>\n",
       "      <th>r_at_2</th>\n",
       "      <th>f1_at_2</th>\n",
       "      <th>...</th>\n",
       "      <th>p_at_20</th>\n",
       "      <th>r_at_20</th>\n",
       "      <th>f1_at_20</th>\n",
       "      <th>p_at_30</th>\n",
       "      <th>r_at_30</th>\n",
       "      <th>f1_at_30</th>\n",
       "      <th>p_at_50</th>\n",
       "      <th>r_at_50</th>\n",
       "      <th>f1_at_50</th>\n",
       "      <th>auc-roc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AB</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>{'algorithm': 'SAMME.R', 'n_estimators': 100}</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>0.961451</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.959230</td>\n",
       "      <td>0.026808</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903996</td>\n",
       "      <td>0.252730</td>\n",
       "      <td>0.395023</td>\n",
       "      <td>0.874943</td>\n",
       "      <td>0.366925</td>\n",
       "      <td>0.517025</td>\n",
       "      <td>0.821084</td>\n",
       "      <td>0.573888</td>\n",
       "      <td>0.675584</td>\n",
       "      <td>0.674833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 0.1, 'penalty': 'l1'}</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0.944186</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.027063</td>\n",
       "      <td>0.925754</td>\n",
       "      <td>0.026983</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876303</td>\n",
       "      <td>0.255833</td>\n",
       "      <td>0.396043</td>\n",
       "      <td>0.851737</td>\n",
       "      <td>0.372963</td>\n",
       "      <td>0.518766</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.584229</td>\n",
       "      <td>0.675476</td>\n",
       "      <td>0.680457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>LR</td>\n",
       "      <td>LogisticRegression(C=10, class_weight=None, du...</td>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.025901</td>\n",
       "      <td>0.050445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906654</td>\n",
       "      <td>0.244006</td>\n",
       "      <td>0.384526</td>\n",
       "      <td>0.876561</td>\n",
       "      <td>0.353879</td>\n",
       "      <td>0.504204</td>\n",
       "      <td>0.838358</td>\n",
       "      <td>0.564116</td>\n",
       "      <td>0.674424</td>\n",
       "      <td>0.665552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name                                              model  \\\n",
       "33          AB  (DecisionTreeClassifier(class_weight=None, cri...   \n",
       "52          LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "104         LR  LogisticRegression(C=10, class_weight=None, du...   \n",
       "\n",
       "                                        parameters train_test_split_threshold  \\\n",
       "33   {'algorithm': 'SAMME.R', 'n_estimators': 100}                 2013-07-01   \n",
       "52                     {'C': 0.1, 'penalty': 'l1'}                 2013-01-01   \n",
       "104                      {'C': 1, 'penalty': 'l1'}                 2012-07-01   \n",
       "\n",
       "       p_at_1    r_at_1   f1_at_1    p_at_2    r_at_2   f1_at_2    ...     \\\n",
       "33   0.961451  0.013420  0.026470  0.959230  0.026808  0.052158    ...      \n",
       "52   0.944186  0.013728  0.027063  0.925754  0.026983  0.052438    ...      \n",
       "104  0.963415  0.012950  0.025557  0.963415  0.025901  0.050445    ...      \n",
       "\n",
       "      p_at_20   r_at_20  f1_at_20   p_at_30   r_at_30  f1_at_30   p_at_50  \\\n",
       "33   0.903996  0.252730  0.395023  0.874943  0.366925  0.517025  0.821084   \n",
       "52   0.876303  0.255833  0.396043  0.851737  0.372963  0.518766  0.800500   \n",
       "104  0.906654  0.244006  0.384526  0.876561  0.353879  0.504204  0.838358   \n",
       "\n",
       "      r_at_50  f1_at_50   auc-roc  \n",
       "33   0.573888  0.675584  0.674833  \n",
       "52   0.584229  0.675476  0.680457  \n",
       "104  0.564116  0.674424  0.665552  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets obtain the best model for each train/test set\n",
    "\n",
    "#indices of rows that have max auc for each train/test set\n",
    "idx = results.groupby(['train_test_split_threshold'])['auc-roc'].transform(max) == results['auc-roc']\n",
    "\n",
    "#display best models\n",
    "best_auc = results[idx]\n",
    "best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare all models performance at different train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each model, find the set of parameters that work the best in each train/test set\n",
    "\n",
    "#In this dataframe we will save the best model for each type of model (ex 1 LR, 1 RF..), whichever perfomed the best in each train/test set\n",
    "best_models= pd.DataFrame()\n",
    "\n",
    "for model in models_to_run:\n",
    "  #Filter data selecting only rows of this specific model\n",
    "  results_of_model = results[results[\"model_name\"]==model]  \n",
    "  #For each train/test set, find index of best model (parameters)\n",
    "  idx = results_of_model.groupby(['train_test_split_threshold'])['auc-roc'].transform(max) == results_of_model['auc-roc']\n",
    "    \n",
    "  #Grab those results based on index\n",
    "  best_model = results_of_model[idx]\n",
    "  #Append it to final list\n",
    "  best_models=best_models.append(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Clear plot\n",
    "plt.clf()\n",
    "\n",
    "#Create plot and axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "#Create lines for each model\n",
    "for model in models_to_run:\n",
    "  ax1.plot( 'train_test_split_threshold', 'auc-roc', data=best_models[best_models['model_name']==model], label=model, marker='o', markersize=6, linewidth=4,alpha=0.6)\n",
    "\n",
    "#Show legends\n",
    "plt.legend()\n",
    "\n",
    "#Set axis labels\n",
    "ax1.set_xlabel('Train/test set split')\n",
    "ax1.set_ylabel('AUC-ROC',)\n",
    "\n",
    "#Invert x_axis so as to show from earliest to latest\n",
    "ax1.invert_xaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
